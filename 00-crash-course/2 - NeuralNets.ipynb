{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network refresher - part 2 - auto differentiation engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From first principle \n",
    "\n",
    "A function of a real variable $f(x)$ is differentiable at a point $a$ of its domain, if its domain contains an open interval $I$ containing $a$ and the limit\n",
    "$$ L= \\lim_{h\\to0} \\frac{f(a+h) - f(a)}{h} $$\n",
    "exists. This means that for every positive real number $\\epsilon$ (even very small), there exists a positive real number $\\delta$ such that, for every such $h$ such that $|h|<\\delta$ and $h\\neq 0$, then\n",
    "\n",
    "$$ \\left| L - \\lim_{h\\to0} \\frac{f(a+h) - f(a)}{h} \\right| < \\epsilon$$\n",
    "\n",
    "In simple words the conditions for a limit to exist are:\n",
    "\n",
    "1. Approaching the Same Value from Both Sides: The function must approach the same value as you get closer to a specific point from both the left and the right. If the left-hand limit and the right-hand limit are different, the overall limit does not exist.\n",
    "\n",
    "2. Finite Value: The value that the function approaches must be a finite number. If the function goes to infinity or negative infinity, the limit does not exist.\n",
    "\n",
    "3. No Oscillations: The function should not oscillate wildly as it approaches the point. If it keeps jumping between different values, the limit does not exist.\n",
    "\n",
    "If the above conditions for the limit hold true, the value given by the formula is the first derivative $f'(x)$ of the function $f(x)$ at point $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)=3x^2-4x+5$\n",
    "\n",
    "By the [rules](https://en.wikipedia.org/wiki/Differentiation_rules) of differentiation \n",
    "\n",
    "$f'(x) = 6x - 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25) # [-5, -4.75, -4.5, ..., 4.75]\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually, the derivative at a few points are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.00000001 # A very small number, well sort of :)\n",
    "x = [3.0, 2.0, 0.0, -1.0, 2/3]\n",
    "print((f(x[0]+h)-f(x[0]))/h) # From the first principle formula\n",
    "print((f(x[1]+h)-f(x[1]))/h) # or we manually calculated the derivative to be 6x - 4\n",
    "print((f(x[2]+h)-f(x[2]))/h) # So the derivative at x = 0 is - 4\n",
    "print((f(x[3]+h)-f(x[3]))/h) # The derivate at x = -1 is 10\n",
    "print((f(x[4]+h)-f(x[4]))/h) # The derivate is 0 at x = 2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note above results have numerical precision error that is not there if we calculate analytically\n",
    "This error will come up time and time again. If you add too many zeros for example, the internal representation of the numbers are messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.00000000000000000001\n",
    "print((f(x[0]+h)-f(x[0]))/h) # x[0] is 3.0, overflow happens\n",
    "h = 0.000001 # Thus h can't be too small in our calculation\n",
    "print((f(x[0]+h)-f(x[0]))/h) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us consider another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10\n",
    "d = a*b + c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ab+c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.0001\n",
    "\n",
    "# inputs\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10\n",
    "\n",
    "d1 = a*b + c\n",
    "a  = a + h # nudging a by a small amount\n",
    "d2 = a*b + c\n",
    "\n",
    "print('d1 = ', d1) # a*b + c\n",
    "print('d2 = ', d2) # (a+h)*b + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b$ is a negative number. Adding $h$ to $a$ slightly increases the value of $a$. Since $b$ is negative, we are adding, overall less to $c$ which brings the value of the result down by a tiny amount\n",
    "This indicates the slope (change) would be negative hence the result went down in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('slope = ', (d2-d1)/h) # f(a+h) - f(a) / h, here we are differentiating with respect to a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly if we now increase the value of $b$ slightly, we see the slope is positive, indicating the result has increased in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.0001\n",
    "\n",
    "# inputs\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10\n",
    "\n",
    "d1 = a*b + c\n",
    "b  = b + h # nudging a by a small amount\n",
    "d2 = a*b + c\n",
    "\n",
    "print('d1 = ', d1) # a*b + c\n",
    "print('d2 = ', d2) # a*(b+h) + c\n",
    "print('slope = ', (d2-d1)/h) # f(b+h) - f(b) / h, here we are differentiating with respect to b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks are massive objects in computer memory. To store them efficiently, we need some kind of data structure\n",
    "### Let us now look at one such implementation, the overall structure and functionality of the following class closely resembles libraries like pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __repr__(self): # returns a representation of the object when we print it\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "a = Value(2.0)\n",
    "a # internally calls a.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to support mathematical operations in our Value class\n",
    "Let us again modify the class to add some methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __repr__(self): # returns a representation of the object when we print it\n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self, other): # python internally calls this method when we use the + operator\n",
    "        out = Value(self.data + other.data)\n",
    "        return out\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "a + b # internally calls a.__add__(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great addition works now. Now we implement multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __repr__(self): # returns a representation of the object when we print it\n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self, other): # python internally calls this method when we use the + operator\n",
    "        out = Value(self.data + other.data)\n",
    "        return out\n",
    "    def __mul__(self, other): # python internally calls this method when we use the * operator\n",
    "        out = Value(self.data * other.data)\n",
    "        return out\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10)\n",
    "a * b # internally calls a.__mul__(b)\n",
    "## and then\n",
    "a*b + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the same as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.__mul__(b).__add__(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great. Neural netoworks are big graph like expressions. Thus we need some kind of connective tissue which binds all the Value objects together into a big graph structure\n",
    "### Let us now modify the Value class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=()): # _children is a private variable with default value of empty tuple\n",
    "        self.data = data\n",
    "        self._prev = set(_children) # set of previous nodes\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self, other): \n",
    "        \n",
    "        out = Value(self.data + other.data, (self, other)) # feeding the cause of result as new node's children\n",
    "        return out\n",
    "    def __mul__(self, other): \n",
    "        \n",
    "        out = Value(self.data * other.data, (self, other)) # new node's children are self and other\n",
    "        return out\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "e = a*b\n",
    "c = Value(10)\n",
    "d = e + c\n",
    "print(d)\n",
    "print(d._prev) # one child for c and one child for a*b i.e e\n",
    "print(e._prev) # a and b are the parents of e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we know what children created a specific Value. But we don't know what operation created them. Thus we modify the class once again to keep track of the operator when new values are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''): # _op is empty for leaves\n",
    "        self.data = data\n",
    "        self._prev = set(_children) # set of previous nodes\n",
    "        self._op = _op # operation performed to get this node\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self, other): \n",
    "        \n",
    "        out = Value(self.data + other.data, (self, other), '+') # feeding the cause of result as new node's children\n",
    "        return out\n",
    "    def __mul__(self, other): \n",
    "        \n",
    "        out = Value(self.data * other.data, (self, other), '*') # new node's children are self and other\n",
    "        return out\n",
    "\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "e = a*b; e.label = 'e'\n",
    "c = Value(10, label='c')\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d*f; L.label = 'L'\n",
    "print(L)\n",
    "print(f\"L._prev: {L._prev}, L._op = {L._op}\") # one child for d and one child for f\n",
    "print(f\"d._prev: {d._prev}, d._op = {d._op}\") # one child for c and one child for a*b i.e e\n",
    "print(f\"r._prev: {e._prev}, r._op: {e._op}\") # a and b are the parents of e\n",
    "print(f\"a._op: {a._op}\") # a is a leaf node, so no operation performed on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Situation is getting out of hand. We need some kind of visual representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in the graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir':'LR'}) # Left to right graph\n",
    "\n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    dot.node(name=uid, label= \"{ %s | data %.4f }\" % (n.label, n.data), shape='record') \n",
    "\n",
    "    if n._op: # will work for non empty _op for every node, e.g for e, d, L \n",
    "      dot.node(name=uid + n._op, label= n._op ) # creating a node for the operation\n",
    "      dot.edge(uid + n._op, uid) # connecting the operation (oval) node to the Value [rectangle] node\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op) # connecting the parent to the child with the operation, if there is any\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are interested in finding out how much does the loss change if we change any of the nodes in the graph\n",
    "### I.e We need to find\n",
    "### $\\frac{\\partial{L}}{\\partial{d}}$, $\\frac{\\partial{L}}{\\partial{f}}$, $\\frac{\\partial{L}}{\\partial{e}}$\n",
    "Finding this out is done through a process known as backpropagation\\\n",
    "$a,b,c$ are inputs. And inputs are fixed. Thus \n",
    "### $\\frac{\\partial{L}}{\\partial{a}} = 0$, $\\frac{\\partial{L}}{\\partial{b}}=0$, $\\frac{\\partial{L}}{\\partial{c}}=0$\n",
    "### Let us modify the Value class once again to add another property, which holds the derivative, or rather the gradient of the loss $L$ with respect to each of the Value nodes\n",
    "\n",
    "We also modify our graph visualisation methods, so we see a gradient corresponding to the Values in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in the graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir':'LR'}) # Left to right graph\n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    dot.node(name=uid, label= \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')  # added grad to the node \n",
    "\n",
    "    if n._op: # will work for non empty _op for every node, e.g for e, d, L \n",
    "      dot.node(name=uid + n._op, label= n._op ) # creating a node for the operation\n",
    "      dot.edge(uid + n._op, uid) # connecting the operation (oval) node to the Value [rectangle] node\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op) # connecting the parent to the child with the operation, if there is any\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''): # _op is empty for leaves\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # gradient of the node, initialized to 0\n",
    "        # 0 means no effect. So we assume at initialization, every value does not effect the output\n",
    "        self._prev = set(_children) \n",
    "        self._op = _op \n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self, other): \n",
    "        \n",
    "        out = Value(self.data + other.data, (self, other), '+') \n",
    "        return out\n",
    "    def __mul__(self, other): \n",
    "        \n",
    "        out = Value(self.data * other.data, (self, other), '*') \n",
    "        return out\n",
    "\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10, label='c')\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d*f; L.label = 'L'\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let us fill in the gradients manually and do backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How much does $L$ change if we take the derivative of $L$ with respect to itself?\n",
    "### 2. What about $a$?\n",
    "### 3. What about $d$?\n",
    "### 4. What about other Values?\n",
    "\n",
    "Let us test it out with the first principle formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dL():\n",
    "  h = 0.0001\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L2 = L.data + h # nudging L2 by a small amount\n",
    "\n",
    "  print(f\"Derivative of L with respect to itself: {(L2 - L1)/h}\")\n",
    "\n",
    "def dL_da():  \n",
    "  # Lets do it with respect to a\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "\n",
    "  a = Value(2.0 + h, label='a') # nudging a by a small amount\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L2 = L.data\n",
    "\n",
    "  print(f\"Derivative of L with respect to a: {(L2 - L1)/h}\")\n",
    "\n",
    "def dL_db():\n",
    "  # Lets do it with respect to b\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0 + h, label='b') # nudging b by a small amount\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L2 = L.data\n",
    "\n",
    "  print(f\"Derivative of L with respect to b: {(L2 - L1)/h}\")\n",
    "\n",
    "def dL_dc():\n",
    "  # Lets do it with respect to c\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10 + h, label='c') # nudging c by a small amount\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L2 = L.data\n",
    "\n",
    "  print(f\"Derivative of L with respect to c: {(L2 - L1)/h}\")\n",
    "\n",
    "def dL_df():\n",
    "  # Lets do it with respect to f\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0 + h, label='f') # nudging f by a small amount\n",
    "  L = d*f; L.label = 'L'\n",
    "  L2 = L.data\n",
    "\n",
    "  print(f\"Derivative of L with respect to f: {(L2 - L1)/h}\")\n",
    "\n",
    "def dL_dd():\n",
    "  # Lets do it with respect to d\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  d = Value(d.data + h, label='d') # nudging d by a small amount\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L2 = L.data\n",
    "\n",
    "  print(f\"Derivative of L with respect to d: {(L2 - L1)/h}\")\n",
    "\n",
    "def dL_de():\n",
    "  # Lets do it with respect to e\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10, label='c')\n",
    "  e = a*b; e.label = 'e'; e.data += h # nudging e by a small amount\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d*f; L.label = 'L'\n",
    "  L2 = L.data\n",
    "\n",
    "  print(f\"Derivative of L with respect to e: {(L2 - L1)/h}\")\n",
    "\n",
    "def derivative():\n",
    "  dL_dL()\n",
    "  dL_da()\n",
    "  dL_db()\n",
    "  dL_dc()\n",
    "  dL_dd()\n",
    "  dL_de()\n",
    "  dL_df()\n",
    "\n",
    "derivative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can manually set $L.grad=1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.grad = 1.0 # setting the gradient of L to 1\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We know $L=d\\times f$, and we are interested to find how much does L change when we change d, i.e $\\frac{dL}{dd}$. By multiplication rule of derivatives, we can calculate $\\frac{dL}{dd} = \\frac{d\\ (f\\times d)}{dd} = f$\n",
    "\n",
    "### Or from the first principles $\\frac{dL}{dd} = \\frac{f(x+h) - f(x)}{h}$\n",
    "\n",
    "### $ \\text{or ‎ ‎ ‎ ‎ ‎ ‎  } \\frac{ (d+h)\\times f - d\\times f}{h}\\ \\text{‎ ‎ ‎ ‎ } [\\because f(x) = d\\times f, f(x+h) = (d+h)\\times f]$\n",
    "\n",
    "### $\\text{or ‎ ‎ }\\frac{df + dh - df}{h}\\\\ \\text{or ‎ ‎ ‎   }\\frac{hf}{h}\\\\ \\text{or ‎ ‎ ‎ ‎ ‎ }f $\n",
    "\n",
    "### Thus $f.grad$ will be the value of $d$\n",
    "### Similarly, $d.grad$ will be the value of $f$\n",
    "\n",
    "### $\\text{Hence we can say that a multiplication operation just copies the value of other's data in current value's gradient}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.grad = d.data # 4.0\n",
    "d.grad = f.data # -2.0\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What would be the contribution of change in c to the change in d? In other words what would $\\frac{dd}{dc}$ be?\n",
    "\n",
    "Again from the first principle we can derive,\n",
    "\n",
    "### $d = c+e$\n",
    "\n",
    "### $\\frac{((c+h)+e) - (c+e)}{h} \\text{‎ ‎ ‎ ‎ } [\\because f(x+h) = (c+h)+e]$\n",
    "\n",
    "### $\\frac{dd}{dc} = 1.0$, and by symmetry,  $\\frac{dd}{de} = 1.0$\n",
    "\n",
    "### However these are local gradients. I.e it is not the gradient of $L$ with respect to $c$ and $e$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we want to know what is the effect of change in $c$ as how much it changes $L$. \n",
    "### What we know: $\\frac{dL}{dd}$, $\\frac{dd}{dc}$ and $\\frac{dd}{de}$\n",
    "### What we want: $\\frac{dL}{dc}$ and $\\frac{dL}{de}$\n",
    "\n",
    "### From chain rule we know: \n",
    "\n",
    "### - When a variable $z$ depends on a variable $y$, which in turn depends on another variable $x$, change in $z$ with respect to change in $x$ is given by:\n",
    "### $\\frac{dz}{dx}=\\frac{dz}{dy}\\cdot\\frac{dy}{dx}$\n",
    "\n",
    "### In simple words: If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels $2\\times 4 = 8$ times as fast as the man\n",
    "\n",
    "### Hence the gradients $\\frac{dL}{dc}$ and $\\frac{dL}{de}$ can be written as:\n",
    "\n",
    "### $\\frac{dL}{dc} =\\frac{dL}{dd} \\times \\frac{dd}{dc} $ and   $\\frac{dL}{de} = \\frac{dL}{dd} \\times \\frac{dd}{de} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.grad = d.grad * 1.0\n",
    "e.grad = d.grad * 1.0\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note\n",
    "\n",
    "### Since we calculated the local gradient of a plus node to be always $1$. We say all a plus operator does, is route the gradients from parent to its children.\n",
    "### Thus $\\frac{dL}{dd} \\times \\frac{dd}{dc} $ will be just $\\frac{dL}{dd}$\n",
    "\n",
    "### Great. Similarly we apply chain rule one more time\n",
    "### We want $\\frac{dL}{da}$, we know $\\frac{dL}{de}$ (we just calculated) and $\\frac{de}{da}$ (which is the local gradient)\n",
    "### $\\frac{de}{da} = b \\text{‎ ‎ } [\\because e=a\\cdot b ]$ \n",
    "### By chain rule $\\frac{dL}{da} = \\frac{dL}{de}\\cdot \\frac{de}{da}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad = e.grad * b.data\n",
    "# similarly\n",
    "b.grad = e.grad * a.data\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we just did, was the manual backpropagation of gradients from the end (Loss function result) to the beginning (inputs)\n",
    "## What will happen if we nudge the leaf nodes by a tiny amount towards their gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "a.data += h*a.grad # nudging a.data by a small amount\n",
    "b.data += h*b.grad # nudging b.data by a small amount\n",
    "c.data += h*c.grad # nudging c.data by a small amount\n",
    "f.data += h*f.grad # nudging f.data by a small amount\n",
    "# Also don't forget to do the forward pass again\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "L = d*f; L.label = 'L'\n",
    "# Note because e,d and L are dependent on a,b,c and f, we need to update them as well\n",
    "# And we lost their gradient values, so we need to do the backward pass again to find the new gradients\n",
    "print(L.data)\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We just performed a single optimisation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let us look at a more practical example of a neuron\n",
    "\n",
    "\n",
    "\n",
    "| Biological Neuron | Mathematical object Neuron | Neural Network |\n",
    "|---------|---------|---------|\n",
    "| ![nerve](./img/nervous-system.jpg) | ![neuron](./img/neuron.jpeg)  | ![neuralNet](./img/neural-network.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us look at an activation function: $tanh$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2)))\n",
    "plt.grid() # Activation function tanh squishes the input between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron b\n",
    "b = Value(6.7, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "# we still can't do n.tanh() because we haven't implemented the tanh function in our Value class\n",
    "draw_dot(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $n$ here is the ouput before the activation function. But right now our Value class doesn't support any kind of activation function. Let's add $tanh$ to our value class\n",
    "\n",
    "Remember $tanh$ is defined as (One of the definitions) $$\\frac{e^{2x}-1}{e^{2x}+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''): # _op is empty for leaves\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # gradient of the node, initialized to 0\n",
    "        # 0 means no effect. So we assume at initialization, every value does not effect the output\n",
    "        self._prev = set(_children) \n",
    "        self._op = _op \n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    def __add__(self, other): \n",
    "        \n",
    "        out = Value(self.data + other.data, (self, other), '+') \n",
    "        return out\n",
    "    def __mul__(self, other): \n",
    "        \n",
    "        out = Value(self.data * other.data, (self, other), '*') \n",
    "        return out\n",
    "    def tanh(self): # as long as we know how to calculate the local gradient of tanh, we can implement it\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh') # unary operation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron b\n",
    "b = Value(6.881373, label='b') # specific value to make the the gradients nice\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we start backpropagating\n",
    "\n",
    "Derivative of $o$ with respect to itself is $1$.\n",
    "What is the derivative of $o$ with respect to $n$?\n",
    "\n",
    "$$\\frac{do}{dn}$$\n",
    "\n",
    "Derivative of $tanh(x)$ is given by $$1-tanh^2(x)$$\n",
    "But $tanh=o$. Therefore \n",
    "$$\\frac{do}{dn}=\\frac{d(tanh(n))}{dn}=1-tanh^2(x)=1-o^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0 # setting the gradient of o to 1\n",
    "n.grad = o.grad * (1 - o.data**2) # tanh'(x) = 1 - tanh^2(x)\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly for the other nodes\n",
    "b.grad = n.grad * 1.0\n",
    "x1w1x2w2.grad = n.grad * 1.0\n",
    "x2w2.grad = x1w1x2w2.grad * 1.0\n",
    "x1w1.grad = x1w1x2w2.grad * 1.0\n",
    "w2.grad = x2w2.grad * x2.data\n",
    "w1.grad = x1w1.grad * x1.data\n",
    "x2.grad = x2w2.grad * w2.data\n",
    "x1.grad = x1w1.grad * w1.data\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are multiplying $x2$ and $w2$, and since $x2$ is $0$, it indicates that no matter how much we wiggle or change $w2$, $w2$ has no contribution for the final output to change because it is multiplied by $x2$ which is $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "### Differentiating manually through the graph is extremely tedious. What we want is to do the differentiation automatically. We change our value class once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''): # _op is empty for leaves\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None # empty function by default to calculate the gradient of the node\n",
    "        self._prev = set(_children) \n",
    "        self._op = _op \n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other): \n",
    "        out = Value(self.data + other.data, (self, other), '+') \n",
    "        # we want to take out's grad and propagate it to self's grad and other's grad\n",
    "        # in general, we multiply the local derivative with the derivative which has propagated\n",
    "        # from the last (rightmost) node\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad # 1 is the local derivative of the addition operation\n",
    "            other.grad += 1.0 * out.grad # out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):     \n",
    "        out = Value(self.data * other.data, (self, other), '*') \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad # other.data is the local derivative of the multiplication operation\n",
    "            other.grad += self.data * out.grad # self.data is the local derivative of the multiplication operation\n",
    "            # out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self): \n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh') \n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad # 1 - tanh^2(x) is the local derivative of tanh, out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redifining the graph with the new Value class\n",
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron b\n",
    "b = Value(6.881373, label='b') # specific value to make the the gradients nice\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to call the `_backward()` function in the right order for all the nodes. Note: $o$ should have gradient = $1$ (as our base case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0 # setting the gradient of o to 1\n",
    "o._backward() # calculating the gradient of all the nodes\n",
    "n._backward()\n",
    "b._backward() # empty lambda function, so nothing happens\n",
    "x1w1x2w2._backward()\n",
    "x2w2._backward()\n",
    "x1w1._backward()\n",
    "w2._backward()\n",
    "w1._backward()\n",
    "x2._backward()\n",
    "x1._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We see that the gradient can't propagate backwards if all of its dependencies are not calculated first. For a graph this can be achieved using a topological sort.\n",
    "### Another example for this kind of ordering: Suppose you want to install a package $xyz$ from the package manager. But $xyz$ needs package $abc$ and $hij$ to be installed first. Further, package $abc$ needs another package $pqr$ to be installed first. So the correct ordering of the packages to be installed would be\n",
    "\n",
    " $ \\{pqr \\rarr abc\\ ||\\ hij\\} \\rarr xyz$\n",
    "\n",
    "In simple terms, think of it as a flat 1D ordering of the vertices of a 2D graph\n",
    "\n",
    "| Topological sort of a Directed Asyclic Graph |\n",
    "|----------------------------------------------|\n",
    "|       ![topo-sort](./img/topo-sort.png)          |\n",
    "\n",
    "Now we want to implement topological sort in our Value class. In a topological sort of a graph (or rather here, a Directed Asyclic Graph, DAG), all the edges go from left to right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topological sort\n",
    "topo = []\n",
    "visited = set()\n",
    "def build_topo(v):\n",
    "  if v not in visited:\n",
    "    visited.add(v)\n",
    "    for child in v._prev:\n",
    "        build_topo(child)\n",
    "    topo.append(v)\n",
    "build_topo(o)\n",
    "# a vertex is only going to be on the list, if all its children are already on the list\n",
    "print('Topological order:', end=' ')\n",
    "for v in topo:\n",
    "    print(v.label, end=', ') # note we need to do backward pass on the reverse order of this list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with all the building blocks in place, let's modify the Value class one more time to include this automatic backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''): # _op is empty for leaves\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None # empty function by default to calculate the gradient of the node\n",
    "        self._prev = set(_children) \n",
    "        self._op = _op \n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other): \n",
    "        out = Value(self.data + other.data, (self, other), '+') \n",
    "        # we want to take out's grad and propagate it to self's grad and other's grad\n",
    "        # in general, we multiply the local derivative with the derivative which has propagated\n",
    "        # from the last (rightmost) node\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad # 1 is the local derivative of the addition operation\n",
    "            other.grad += 1.0 * out.grad # out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):     \n",
    "        out = Value(self.data * other.data, (self, other), '*') \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad # other.data is the local derivative of the multiplication operation\n",
    "            other.grad += self.data * out.grad # self.data is the local derivative of the multiplication operation\n",
    "            # out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self): \n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh') \n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad # 1 - tanh^2(x) is the local derivative of tanh, out.grad is the derivative propagated from the last node\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redifining the graph with the new Value class\n",
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron b\n",
    "b = Value(6.881373, label='b') # specific value to make the the gradients nice\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1x2w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallels with pytorch\n",
    "\n",
    "## PyTorch is a very popular library to build neural networks and train them. Let's look at how pytorch's semantics matches with what we wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x1 = torch.Tensor([2.0]).double();   x1.requires_grad = True # by default requires_grad is False for efficiency reasons\n",
    "x2 = torch.Tensor([0.0]).double();   x2.requires_grad = True # we need to set it to True to calculate the gradients\n",
    "w1 = torch.Tensor([-3.0]).double(); w1.requires_grad = True # main difference between our library and pytorch is pytorch is much more efficient\n",
    "w2 = torch.Tensor([1.0]).double();  w2.requires_grad = True # it can do operations in parallel, vectorized operations\n",
    "b = torch.Tensor([6.8813735870195432]).double(); b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "\n",
    "o.backward()\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(\"x2\", x2.grad.item())\n",
    "print(\"w2\", w2.grad.item())\n",
    "print(\"x1\", x1.grad.item())\n",
    "print(\"w1\", w1.grad.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
