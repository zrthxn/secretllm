{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformer (T5-like Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement an encoder-decoder transformer model similar to T5. The model is designed for sequence-to-sequence tasks like translation or summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (\"mps\" if you're using an M series mac):\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample vocabulary and data\n",
    "vocab = ['[PAD]', '[BOS]', '[EOS]', 'i', 'like', 'eating', 'apples', 'bananas', 'fruits']\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens\n",
    "There are some different tokens you might stumble across when dealing with inputs in NLP. \n",
    "- The [PAD] Token acts as a padding if a sentence does not have the desired fixed length. \n",
    "- The [BOS] Token indicates the Beginning of the Sentence. \n",
    "- The [EOS] Token indicates the End of the Sentence. \n",
    "- The [CLS] Token represents Sentence Level Classification. \n",
    "- The [SEP] Token represents Separation of Sentences (used by BERT). \n",
    "- The [UNK] Token represents OOB-Tokens, meaning unknown Tokens that are not included in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input-output pairs (e.g., paraphrasing)\n",
    "input_sentences = [\n",
    "    ['[BOS]', 'i', 'like', 'eating', 'apples', '[EOS]'],\n",
    "    ['[BOS]', 'i', 'like', 'eating', 'bananas', '[EOS]']\n",
    "]\n",
    "output_sentences = [\n",
    "    ['[BOS]', 'i', 'like', 'fruits', '[EOS]'],\n",
    "    ['[BOS]', 'i', 'like', 'fruits', '[EOS]']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_data(sentences):\n",
    "    inputs = []\n",
    "    for sent in sentences:\n",
    "        input_ids = [word_to_idx[word] for word in sent]\n",
    "        inputs.append(input_ids)\n",
    "    return torch.tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "encoder_inputs = prepare_data(input_sentences).to(device)\n",
    "decoder_inputs = prepare_data([['[BOS]'] + sent[1:] for sent in output_sentences]).to(device)\n",
    "decoder_targets = prepare_data([sent[1:] + ['[PAD]'] for sent in output_sentences]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding (same as before)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dimension, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # TODO: Implement positional encoding\n",
    "        positional_encoding = torch.zeros(max_len, model_dimension)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        frequency_divisor = torch.exp(torch.arange(0, model_dimension, 2).float() * (-np.log(10000.0) / model_dimension))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * frequency_divisor)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * frequency_divisor)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer (same as before)\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dimension, num_attention_heads, dim_feedforward, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # TODO: Implement encoder layer components\n",
    "        self.self_attention = nn.MultiheadAttention(model_dimension, num_attention_heads, dropout=dropout)\n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(model_dimension, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, model_dimension)\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(model_dimension)\n",
    "        self.norm2 = nn.LayerNorm(model_dimension)\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_tensor, src_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        attention_output = self.self_attention(input_tensor, input_tensor, input_tensor, attn_mask=src_mask)[0]\n",
    "        input_tensor = input_tensor + self.dropout1(attention_output)\n",
    "        input_tensor = self.norm1(input_tensor)\n",
    "        # Feedforward network\n",
    "        attention_output = self.linear2(self.dropout(F.relu(self.linear1(input_tensor))))\n",
    "        input_tensor = input_tensor + self.dropout2(attention_output)\n",
    "        input_tensor = self.norm2(input_tensor)\n",
    "        return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dimension, num_attention_heads, dim_feedforward, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # TODO: Implement decoder layer components\n",
    "        self.self_attention = nn.MultiheadAttention(model_dimension, num_attention_heads, dropout=dropout)\n",
    "        self.multihead_attention = nn.MultiheadAttention(model_dimension, num_attention_heads, dropout=dropout)\n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(model_dimension, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, model_dimension)\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(model_dimension)\n",
    "        self.norm2 = nn.LayerNorm(model_dimension)\n",
    "        self.norm3 = nn.LayerNorm(model_dimension)\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, target, memory, target_mask=None, memory_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        # Self-attention\n",
    "        target2 = self.self_attention(target, target, target, attn_mask=target_mask)[0]\n",
    "        target = target + self.dropout1(target2)\n",
    "        target = self.norm1(target)\n",
    "        # Multi-head attention with encoder output\n",
    "        target2 = self.multihead_attention(target, memory, memory, attn_mask=memory_mask)[0]\n",
    "        target = target + self.dropout2(target2)\n",
    "        target = self.norm2(target)\n",
    "        # Feedforward network\n",
    "        target2 = self.linear2(self.dropout(F.relu(self.linear1(target))))\n",
    "        target = target + self.dropout3(target2)\n",
    "        target = self.norm3(target)\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder (same as before)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_attention_heads, vocab_size, dim_feedforward, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        # TODO: Implement encoder components\n",
    "        self.model_dimension = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding_layer = PositionalEncoding(d_model)\n",
    "        # Create multiple encoder layers\n",
    "        encoder_layer = EncoderLayer(d_model, num_attention_heads, dim_feedforward, dropout)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input_sequence, src_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        input_sequence = self.embedding(input_sequence) * np.sqrt(self.model_dimension)\n",
    "        input_sequence = self.positional_encoding_layer(input_sequence)\n",
    "        input_sequence = input_sequence.permute(1, 0, 2)  # (sequence length, batch size, embedding size)\n",
    "        for layer in self.layers:\n",
    "            input_sequence = layer(input_sequence, src_mask)\n",
    "        input_sequence = self.norm(input_sequence)\n",
    "        return input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, model_dimension, num_attention_heads, vocab_size, dim_feedforward, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # TODO: Implement decoder components\n",
    "        self.d_model = model_dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dimension)\n",
    "        self.positional_encoder_layer = PositionalEncoding(model_dimension)\n",
    "        # Create multiple decoder layers\n",
    "        decoder_layer = DecoderLayer(model_dimension, num_attention_heads, dim_feedforward, dropout)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(model_dimension)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        tgt = self.embedding(tgt) * np.sqrt(self.d_model)\n",
    "        tgt = self.positional_encoder_layer(tgt)\n",
    "        tgt = tgt.permute(1, 0, 2)  # (sequence length, batch size, embedding size)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
    "        tgt = self.norm(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Model\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, vocab_size, dim_feedforward, dropout=0.1):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        # TODO: Implement Seq2Seq model components\n",
    "        self.encoder = Encoder(num_layers, d_model, nhead, vocab_size, dim_feedforward, dropout)\n",
    "        self.decoder = Decoder(num_layers, d_model, nhead, vocab_size, dim_feedforward, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(tgt, memory, tgt_mask)\n",
    "        output = self.output_layer(decoder_output.permute(1, 0, 2))  # Back to (batch size, sequence length, vocab size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 2\n",
    "model_dimension = 64\n",
    "num_attention_heads = 4\n",
    "dim_feedforward = 128\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = Seq2SeqModel(num_layers, model_dimension, num_attention_heads, vocab_size, dim_feedforward, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['[PAD]'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks (not used in this simple example)\n",
    "source_attention_mask = None\n",
    "target_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.3137\n",
      "Epoch 20/100, Loss: 0.0894\n",
      "Epoch 30/100, Loss: 0.0484\n",
      "Epoch 40/100, Loss: 0.0372\n",
      "Epoch 50/100, Loss: 0.0279\n",
      "Epoch 60/100, Loss: 0.0227\n",
      "Epoch 70/100, Loss: 0.0197\n",
      "Epoch 80/100, Loss: 0.0160\n",
      "Epoch 90/100, Loss: 0.0147\n",
      "Epoch 100/100, Loss: 0.0135\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # TODO: Forward pass\n",
    "    outputs = model(encoder_inputs, decoder_inputs, source_attention_mask, target_mask)\n",
    "    # Reshape outputs and targets\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets = decoder_targets.view(-1)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generate output sequence\n",
    "def generate_sequence(model, input_sentence, max_length=10):\n",
    "    model.eval()\n",
    "    input_ids = [word_to_idx.get(word, word_to_idx['[PAD]']) for word in input_sentence]\n",
    "    source_tensor = torch.tensor([input_ids]).to(device)\n",
    "    memory = model.encoder(source_tensor)\n",
    "    target_tokens = [word_to_idx['[BOS]']]\n",
    "    for _ in range(max_length):\n",
    "        target = torch.tensor([target_tokens]).to(device)\n",
    "        output = model.decoder(target, memory)\n",
    "        output = model.output_layer(output.permute(1, 0, 2))\n",
    "        next_token = output.argmax(-1)[:, -1].item()\n",
    "        target_tokens.append(next_token)\n",
    "        if next_token == word_to_idx['[EOS]']:\n",
    "            break\n",
    "    output_sentence = [idx_to_word[idx] for idx in target_tokens]\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ['[BOS]', 'i', 'like', 'eating', 'apples', '[EOS]']\n",
      "Generated Sentence: ['[BOS]', 'i', 'like', 'fruits', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_sentence = ['[BOS]', 'i', 'like', 'eating', 'apples', '[EOS]']\n",
    "generated_sentence = generate_sequence(model, test_sentence)\n",
    "print(\"Input Sentence:\", test_sentence)\n",
    "print(\"Generated Sentence:\", generated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
