{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-Only Transformer (GPT-like Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement a decoder-only transformer model similar to GPT. The model is designed for causal language modeling (Next Token Prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (\"mps\" if you're using an M series mac):\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens\n",
    "There are some different tokens you might stumble across when dealing with inputs in NLP. \n",
    "- The [PAD] Token acts as a padding if a sentence does not have the desired fixed length. \n",
    "- The [BOS] Token indicates the Beginning of the Sentence. \n",
    "- The [EOS] Token indicates the End of the Sentence. \n",
    "- The [CLS] Token represents Sentence Level Classification. \n",
    "- The [SEP] Token represents Separation of Sentences (used by BERT). \n",
    "- The [UNK] Token represents OOB-Tokens, meaning unknown Tokens that are not included in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample vocabulary and data\n",
    "vocab = ['[PAD]', '[BOS]', '[EOS]', 'i', 'like', 'to', 'eat', 'apples', 'bananas', 'fruits']\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "sentences = [\n",
    "    ['[BOS]', 'i', 'like', 'to', 'eat', 'apples', '[EOS]'],\n",
    "    ['[BOS]', 'i', 'like', 'to', 'eat', 'bananas', '[EOS]']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_data(sentences):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for sent in sentences:\n",
    "        input_ids = [word_to_idx[word] for word in sent[:-1]]\n",
    "        target_ids = [word_to_idx[word] for word in sent[1:]]\n",
    "        inputs.append(input_ids)\n",
    "        targets.append(target_ids)\n",
    "    return torch.tensor(inputs), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "inputs, targets = prepare_data(sentences)\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding (same as before)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # TODO: Implement positional encoding\n",
    "        # Create a positional encoding matrix with shape (max_len, d_model)\n",
    "        # Use torch.arange and torch.exp to calculate the positional encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Layer (modified for causal masking)\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # TODO: Implement decoder layer components\n",
    "        # Multi-headed self-attention\n",
    "        \n",
    "        # Feedforward network (linear, dropout, linear)\n",
    "        \n",
    "        # Layer normalization, twice\n",
    "        \n",
    "        # Dropout layers, twice\n",
    "        \n",
    "\n",
    "    def forward(self, target_sequence, tgt_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        \n",
    "        # Feedforward network\n",
    "        \n",
    "        return target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, vocab_size, dim_feedforward, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # TODO: Implement decoder components (Embedding & Positional Encoding)\n",
    "        \n",
    "        # Create multiple decoder layers and norm\n",
    "        \n",
    "\n",
    "    def forward(self, target_sequence, tgt_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        \n",
    "        return target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Model\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, vocab_size, dim_feedforward, dropout=0.1):\n",
    "        super(GPTModel, self).__init__()\n",
    "        # TODO: Implement GPT model components (Decoder & Output Layer)\n",
    "        \n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a causal mask\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    # Causal mask (upper triangular matrix)\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1).type(torch.uint8)\n",
    "    return mask == 1  # In PyTorch, True values are masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 2\n",
    "model_dimension = 64\n",
    "num_attention_heads = 4\n",
    "feedforward_dimension = 128\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = GPTModel(num_layers, model_dimension, num_attention_heads, vocab_size, feedforward_dimension, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['[PAD]'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.7084\n",
      "Epoch 20/100, Loss: 0.2794\n",
      "Epoch 30/100, Loss: 0.1896\n",
      "Epoch 40/100, Loss: 0.1766\n",
      "Epoch 50/100, Loss: 0.1389\n",
      "Epoch 60/100, Loss: 0.1484\n",
      "Epoch 70/100, Loss: 0.1269\n",
      "Epoch 80/100, Loss: 0.1406\n",
      "Epoch 90/100, Loss: 0.1304\n",
      "Epoch 100/100, Loss: 0.1154\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Generate mask\n",
    "    target_sequence_mask = generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "    # TODO: Forward pass\n",
    "    \n",
    "    # Reshape outputs and targets\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets_flat = targets.view(-1)\n",
    "    loss = criterion(outputs, targets_flat)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generate text\n",
    "def generate_text(model, start_tokens, max_length=10):\n",
    "    model.eval()\n",
    "    generated = start_tokens.copy()\n",
    "    input_ids = [word_to_idx.get(word, word_to_idx['[PAD]']) for word in generated]\n",
    "    for _ in range(max_length):\n",
    "        target_tensor = torch.tensor([input_ids]).to(device)\n",
    "        target_sequence_mask = generate_square_subsequent_mask(len(target_tensor[0])).to(device)\n",
    "        output = model(target_tensor, target_sequence_mask)\n",
    "        next_token = output[:, -1, :].argmax(-1).item()\n",
    "        input_ids.append(next_token)\n",
    "        generated.append(idx_to_word[next_token])\n",
    "        if idx_to_word[next_token] == '[EOS]':\n",
    "            break\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Sequence: ['[BOS]', 'i', 'like', 'apples']\n",
      "Generated Sequence: ['[BOS]', 'i', 'like', 'apples', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "# This is how you would inference the model\n",
    "# In this case, we have only 3 sentences in the training data, so the model will not be of use at all\n",
    "start_sequence = ['[BOS]', 'i', 'like', 'apples']\n",
    "generated_sequence = generate_text(model, start_sequence)\n",
    "print(\"Start Sequence:\", start_sequence)\n",
    "print(\"Generated Sequence:\", generated_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
