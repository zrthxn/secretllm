{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-Only Transformer (GPT-like Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement a decoder-only transformer model similar to GPT. The model is designed for causal language modeling (Next Token Prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (\"mps\" if you're using an M series mac):\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens\n",
    "There are some different tokens you might stumble across when dealing with inputs in NLP. \n",
    "- The [PAD] Token acts as a padding if a sentence does not have the desired fixed length. \n",
    "- The [BOS] Token indicates the Beginning of the Sentence. \n",
    "- The [EOS] Token indicates the End of the Sentence. \n",
    "- The [CLS] Token represents Sentence Level Classification. \n",
    "- The [SEP] Token represents Separation of Sentences (used by BERT). \n",
    "- The [UNK] Token represents OOB-Tokens, meaning unknown Tokens that are not included in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample vocabulary and data\n",
    "vocab = ['[PAD]', '[BOS]', '[EOS]', 'i', 'like', 'to', 'eat', 'apples', 'bananas', 'fruits']\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "sentences = [\n",
    "    ['[BOS]', 'i', 'like', 'to', 'eat', 'apples', '[EOS]'],\n",
    "    ['[BOS]', 'i', 'like', 'to', 'eat', 'bananas', '[EOS]']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_data(sentences):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for sent in sentences:\n",
    "        input_ids = [word_to_idx[word] for word in sent[:-1]]\n",
    "        target_ids = [word_to_idx[word] for word in sent[1:]]\n",
    "        inputs.append(input_ids)\n",
    "        targets.append(target_ids)\n",
    "    return torch.tensor(inputs), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "inputs, targets = prepare_data(sentences)\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding (same as before)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # TODO: Implement positional encoding\n",
    "        positional_encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Layer (modified for causal masking)\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # TODO: Implement decoder layer components\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, target_sequence, tgt_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        target2 = self.self_attention(target_sequence, target_sequence, target_sequence, attn_mask=tgt_mask)[0]\n",
    "        target_sequence = target_sequence + self.dropout1(target2)\n",
    "        target_sequence = self.norm1(target_sequence)\n",
    "        # Feedforward network\n",
    "        target2 = self.linear2(self.dropout(F.relu(self.linear1(target_sequence))))\n",
    "        target_sequence = target_sequence + self.dropout2(target2)\n",
    "        target_sequence = self.norm2(target_sequence)\n",
    "        return target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, vocab_size, dim_feedforward, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # TODO: Implement decoder components\n",
    "        self.model_dimension = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        # Create multiple decoder layers\n",
    "        decoder_layer = DecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, target_sequence, tgt_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        target_sequence = self.embedding(target_sequence) * np.sqrt(self.model_dimension)\n",
    "        target_sequence = self.pos_encoder(target_sequence)\n",
    "        target_sequence = target_sequence.permute(1, 0, 2)  # (sequence length, batch size, embedding size)\n",
    "        for layer in self.layers:\n",
    "            target_sequence = layer(target_sequence, tgt_mask)\n",
    "        target_sequence = self.norm(target_sequence)\n",
    "        return target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Model\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, vocab_size, dim_feedforward, dropout=0.1):\n",
    "        super(GPTModel, self).__init__()\n",
    "        # TODO: Implement GPT model components\n",
    "        self.decoder = Decoder(num_layers, d_model, nhead, vocab_size, dim_feedforward, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        decoder_output = self.decoder(tgt, tgt_mask)\n",
    "        output = self.output_layer(decoder_output.permute(1, 0, 2))  # Back to (batch size, sequence length, vocab size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a causal mask\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    # TODO: Implement causal mask\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1).type(torch.uint8)\n",
    "    return mask == 1  # In PyTorch, True values are masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 2\n",
    "model_dimension = 64\n",
    "num_attention_heads = 4\n",
    "feedforward_dimension = 128\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = GPTModel(num_layers, model_dimension, num_attention_heads, vocab_size, feedforward_dimension, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['[PAD]'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.7084\n",
      "Epoch 20/100, Loss: 0.2794\n",
      "Epoch 30/100, Loss: 0.1896\n",
      "Epoch 40/100, Loss: 0.1766\n",
      "Epoch 50/100, Loss: 0.1389\n",
      "Epoch 60/100, Loss: 0.1484\n",
      "Epoch 70/100, Loss: 0.1269\n",
      "Epoch 80/100, Loss: 0.1406\n",
      "Epoch 90/100, Loss: 0.1304\n",
      "Epoch 100/100, Loss: 0.1154\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Generate mask\n",
    "    target_sequence_mask = generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "    # TODO: Forward pass\n",
    "    outputs = model(inputs, target_sequence_mask)\n",
    "    # Reshape outputs and targets\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets_flat = targets.view(-1)\n",
    "    loss = criterion(outputs, targets_flat)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generate text\n",
    "def generate_text(model, start_tokens, max_length=10):\n",
    "    model.eval()\n",
    "    generated = start_tokens.copy()\n",
    "    input_ids = [word_to_idx.get(word, word_to_idx['[PAD]']) for word in generated]\n",
    "    for _ in range(max_length):\n",
    "        target_tensor = torch.tensor([input_ids]).to(device)\n",
    "        target_sequence_mask = generate_square_subsequent_mask(len(target_tensor[0])).to(device)\n",
    "        output = model(target_tensor, target_sequence_mask)\n",
    "        next_token = output[:, -1, :].argmax(-1).item()\n",
    "        input_ids.append(next_token)\n",
    "        generated.append(idx_to_word[next_token])\n",
    "        if idx_to_word[next_token] == '[EOS]':\n",
    "            break\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Sequence: ['[BOS]', 'i', 'like', 'apples']\n",
      "Generated Sequence: ['[BOS]', 'i', 'like', 'apples', '[EOS]']\n"
     ]
    }
   ],
   "source": [
    "# This is how you would inference the model\n",
    "# In this case, we have only 3 sentences in the training data, so the model will not be of use at all\n",
    "start_sequence = ['[BOS]', 'i', 'like', 'apples']\n",
    "generated_sequence = generate_text(model, start_sequence)\n",
    "print(\"Start Sequence:\", start_sequence)\n",
    "print(\"Generated Sequence:\", generated_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
