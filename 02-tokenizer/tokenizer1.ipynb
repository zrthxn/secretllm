{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization is at the heart of much weirdness of LLMs. \n",
    "\n",
    "<img src=\"https://preview.redd.it/i-am-so-proud-of-myself-v0-sm00vxrrwwjd1.jpg?width=1080&crop=smart&auto=webp&s=ba1f33900d10a96521a4ca4a61e923b82e8b2119" />\n",
    "\n",
    "You can play around with different kind of tokenizers here in [this website](https://tiktokenizer.vercel.app/)\n",
    "Check out how code an foreign languages are represented in different tokenizers. Check how spaces in let's say python are handled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strings** are just numbers encoded by a standard text encoding scheme like unicode. To get the unicode number representation for a numebr we can use the `ord` function in python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ord(\"l\"))\n",
    "print([ord(c) for c in \"hi there üëãüèº\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode supports about 150k such numbers. Adding support to all the different characters supported by Unicode will massively increase the input to our language model. Standards can also change. We primarily use unicode (utf-8) encoding because it's fairly concise in terms of the size of the representation for english and it's backwards compatible with ASCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte pair encoding algorithm\n",
    "\n",
    "Feeding raw utf-8 would be really nice but the downside to that is the long context length. Remember our model has a limited amount of memory, thus we need to compress the raw text input into something smaller (ideally variable length) which still retains the same information as our original text. \n",
    "\n",
    "Remember language models have finite context length. If we can somehow compress information into the language model before it can process, we are saving on the limited context length we have\n",
    "\n",
    "The classic idea of text compression (huffman coding) says we put more effort (more characters, more memory) to represent characters which are rare in our sequence and characters or rather sequence of characters which repeat a lot can be represented with shorter symbols or less memory\n",
    "\n",
    "The BPE algorithm follows from the same idea. For example\n",
    "Suppose we have the following string:\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a bute that is not used in the data, such as \"Z\". Now there is the following data and replacement table\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "Then this process is repeated. We keep minting new tokens (symbols) to replace old symbols which repeat frequently\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "(Much like how we expand grammar in formal languages)\n",
    "\n",
    "And finally\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "**Vocabulary** refers to the number of unique symbols we use to represent our text. For decimal number system with base 10 the vocabulary is 10 (0-9) and for english the vocabulary of characters is 26 (a-z). Note that by replacing `Z=aa` we effectively reduced the length of our string but the vocabulary size has increased\n",
    "\n",
    "### The reason it is called Byte Pair Encoding\n",
    "\n",
    "The \"byte pair\" part highlights that the algorithm focuses on frequent pairs of elements (characters or bytes) and merges them iteratively.\n",
    "In modern natural language processing, BPE helps in creating subword-level tokenizations, which are more effective than word-level tokenization, especially for handling rare or out-of-vocabulary words, because it can break them down into smaller, meaningful subunits (subwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"ScaDS.AI (Center for Scalable Data Analytics and Artificial Intelligence) Dresden/Leipzig is a center for Data Science, Artificial Intelligence and Big Data with locations in Dresden and Leipzig. One of five new AI centers in Germany funded under the federal government‚Äôs AI strategy by the Federal Ministry of Education and Research (BMBF) and the Free State of Saxony. Established as a permanent research facility at both locations with strong connections to the local universities: TU Dresden and Leipzig University. Over 60 Principal Investigators, more than 180 employees and up to 12 new AI professorships in Dresden and Leipzig\"\"\"\n",
    "tokens = text.encode('utf-8')\n",
    "tokens = list(map(int, tokens))\n",
    "print('####')\n",
    "print(text)\n",
    "print(\"length of text:\", len(text))\n",
    "print(\"####\")\n",
    "print(tokens)\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = {} # count of each id\n",
    "# for pair in zip(tokens, tokens[1:]):\n",
    "#   print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to count which pairs of characters are occuring the most amount of time. So we write a function for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "  counts = {} # count of each id\n",
    "  for pair in zip(ids, ids[1:]): # iterate over consecutive elements of a list\n",
    "    counts[pair] = counts.get(pair, 0) + 1 # if pair is not in counts, return 0 otherwise increment by 1\n",
    "  return counts\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "print(sorted(((v, k) for k,v in stats.items()), reverse=True)) # sort by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to merge this pair so we write a merge function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # replace all consecutive occurrences of pair with a new idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not a the end and the current id and the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "print(merge([4,3,3,8,1,5], (3,8), 999))\n",
    "print(merge(tokens, top_pair, 637))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 800 # the desired size of the vocabulary\n",
    "token_size = len(tokens) # the current size of the vocabulary\n",
    "num_merges = vocab_size - token_size # the number of merges we need to make\n",
    "ids = list(tokens) # the current vocabulary, so we don't destroy the original list\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get) # get the most common pair by value\n",
    "  idx = token_size + i # the new id \n",
    "  print(f\"merging {pair} to a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx # keep track of the merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokens length: \", len(tokens))\n",
    "print(\"ids length: \", len(ids))\n",
    "print(f\"Compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We just trained our tokenizer how to merge based on some input text\n",
    "\n",
    "### Note that this text is different from the training dataset for our LLM. This is just for the tokenizer\n",
    "Now we wish to encode the ext before feeding it to the language model and decode the text coming out of the language model. \n",
    "### **NOTE:** Despite its name, BPE often works with unicode code points and not byte sequences. [Read here](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "def encode(text):\n",
    "  tokens = list(text.encode('utf-8'))\n",
    "  while len(tokens) > 1:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min ( stats, key=lambda p: merges.get(p, float('inf')) )\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "txt = \"Testing our tokenizer, it should compress this text\"\n",
    "enc = encode(txt)\n",
    "print(f\"Encoded: {enc} \\nLength of sequence of tokens: {len(enc)}, length of original text: {len(txt)}, compression ratio: {len(txt) / len(enc):.2f}X\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "  vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "# decoder\n",
    "def decode(ids):\n",
    "  # given ids, list of integers, return python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode('utf-8', errors='replace') # replace any unknown characters with unicode replacement character\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(encode(\"Is it true for all strings?\"))) # not true for all decodings (If LLM produces garbage, it may not be printable bytewise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some merges should never happen\n",
    "For example, in the GPT2 paper\n",
    "`dog` `dog!` and `dog?` have different representations becuase of merging with the symbols that follow. This is very inefficient. Thus we want to break up the text into different catagories of symbols and in between these catagories, merges should never happen. Let's look at another example using some regular expressions. (see the explaination for this regex [here](https://regexr.com/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "print(re.findall(gpt2pat,\"Hello world how are you? dog dog? dog! dog.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What this does: based on the regular expression rule, it separates out the string into a list of different words. Here dog has only one representation and all the punctuations are separated\n",
    "### The idea is to paralelly process all the items in this list separately by the encoder and then concatenate the results together\n",
    "However, OpenAI didn't just use this naive way of merging tokens by separating it into catagories. For example for `python code` they have some special rules that merge all indentation spaces etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "#GPT2 tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"      hello world!!!  \"))\n",
    "\n",
    "#GPT4 tokenizer\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"      hello world!!!  \"))\n",
    "\n",
    "#Our tokenizer\n",
    "print(encode(\"      hello world!!!  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See also: [Safety issues with tokenization](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
