{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Fine-tuning LLMs","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\nfrom datasets import load_dataset\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-15T19:57:01.996971Z","iopub.execute_input":"2024-04-15T19:57:01.997629Z","iopub.status.idle":"2024-04-15T19:57:20.967654Z","shell.execute_reply.started":"2024-04-15T19:57:01.997585Z","shell.execute_reply":"2024-04-15T19:57:20.966910Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-15 19:57:10.454676: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-15 19:57:10.454792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-15 19:57:10.603248: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load model","metadata":{}},{"cell_type":"code","source":"model_name = 'gpt2'\nmodel = GPT2LMHeadModel.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:20.969665Z","iopub.execute_input":"2024-04-15T19:57:20.970432Z","iopub.status.idle":"2024-04-15T19:57:24.168304Z","shell.execute_reply.started":"2024-04-15T19:57:20.970395Z","shell.execute_reply":"2024-04-15T19:57:24.167448Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7165a6a5d43f4b2daae31d670bd8ae65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012186866c854c05be1634a7afce9758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41d82a31d8b44767b8f7e452f1e6e581"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Load tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:24.169559Z","iopub.execute_input":"2024-04-15T19:57:24.169872Z","iopub.status.idle":"2024-04-15T19:57:25.961596Z","shell.execute_reply.started":"2024-04-15T19:57:24.169846Z","shell.execute_reply":"2024-04-15T19:57:25.960797Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be1105bc61e748378d90e5b3bc0a9c09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725ebf9359134f6098b4f0658f04c341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcc37fd16da64fc3a5c34f36556918d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30b3dc5820740518714742b9a29382f"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Using base model","metadata":{}},{"cell_type":"code","source":"# Place the model in evaluation mode\nmodel.eval()\n\n# Check if CUDA is available and set the device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Your prompt for the recipe\nrecipe_prompt = \"Give me a recipe instructions to make a cake\"\n\n# Tokenize the input, this encodes the prompt to a format that the model can understand\ninputs = tokenizer.encode(recipe_prompt, return_tensors='pt').to(device)\n\n# Generate the output using the model\noutputs = model.generate(inputs, max_length=512, num_return_sequences=1)\n\n# Decode the generated output back to a readable string\nrecipe = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(recipe)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:25.963702Z","iopub.execute_input":"2024-04-15T19:57:25.964040Z","iopub.status.idle":"2024-04-15T19:57:33.689571Z","shell.execute_reply.started":"2024-04-15T19:57:25.964013Z","shell.execute_reply":"2024-04-15T19:57:33.688652Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Give me a recipe instructions to make a cake.\n\nI've been making this for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a while now and I've been really enjoying it. I've been making it for a\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prompt Engineering","metadata":{}},{"cell_type":"code","source":"# Your initial instructions string\ninstructions_string = f\"\"\"give me a recipe to make a chocolate cake\"\"\"\ncomment =\"To make a basic chocolate cake, mix 1 cup sugar, ¾ cup flour, ½ cup cocoa powder, ¾ tsp baking powder, ¾ tsp baking soda, a pinch of salt, 1 egg, ½ cup milk, ¼ cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30-35 minutes. Enjoy your delicious dessert!\"\n\n# Prompt template\nprompt_template = lambda instructions_string, comment: f'''[INST] {instructions_string} \\nInstructions: {comment} \\n[/INST]'''\n\n# Create the prompt\nprompt = prompt_template(instructions_string, comment)\nprint(prompt)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:33.690674Z","iopub.execute_input":"2024-04-15T19:57:33.690981Z","iopub.status.idle":"2024-04-15T19:57:33.697484Z","shell.execute_reply.started":"2024-04-15T19:57:33.690948Z","shell.execute_reply":"2024-04-15T19:57:33.696607Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[INST] give me a recipe to make a chocolate cake \nInstructions: To make a basic chocolate cake, mix 1 cup sugar, ¾ cup flour, ½ cup cocoa powder, ¾ tsp baking powder, ¾ tsp baking soda, a pinch of salt, 1 egg, ½ cup milk, ¼ cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30-35 minutes. Enjoy your delicious dessert! \n[/INST]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenize the input text using the GPT-2 tokenizer\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n# Generate the output using the GPT-2 model\noutputs = model.generate(input_ids=inputs[\"input_ids\"], max_length=512) \n\n# Decode the generated output back to a readable string, removing any special tokens\nrecipe = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(recipe)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:33.698742Z","iopub.execute_input":"2024-04-15T19:57:33.699032Z","iopub.status.idle":"2024-04-15T19:57:38.069370Z","shell.execute_reply.started":"2024-04-15T19:57:33.699009Z","shell.execute_reply":"2024-04-15T19:57:38.068451Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST] give me a recipe to make a chocolate cake \nInstructions: To make a basic chocolate cake, mix 1 cup sugar, ¾ cup flour, ½ cup cocoa powder, ¾ tsp baking powder, ¾ tsp baking soda, a pinch of salt, 1 egg, ½ cup milk, ¼ cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30-35 minutes. Enjoy your delicious dessert! \n[/INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n[INST]\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the pad_token if it's not already set\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Ensure the model and tokenizer are on the same device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Your prompt for the recipe\nprompt = \"Give me a recipe to make a chocolate cake\"\n\n# Tokenize the input text using the GPT-2 tokenizer\ninputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True).to(device)\n\n# Generate the output using the GPT-2 model\noutputs = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    max_new_tokens=50  # Specify how many new tokens to generate beyond the input\n)\n\n# Decode the generated output back to a readable string, removing any special tokens\nrecipe = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(recipe)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:38.070510Z","iopub.execute_input":"2024-04-15T19:57:38.070799Z","iopub.status.idle":"2024-04-15T19:57:38.630161Z","shell.execute_reply.started":"2024-04-15T19:57:38.070750Z","shell.execute_reply":"2024-04-15T19:57:38.629265Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"Give me a recipe to make a chocolate cake\n\nI've been making chocolate cake for a while now, and I've been making it for a long time. I've been making it for a long time, and I've been making it for a long time. I've been making it\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prepare Model for Training","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n\nconfig = GPT2Config.from_pretrained(model_name)\n\nif hasattr(model, 'gradient_checkpointing_enable'):\n    model.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:38.631863Z","iopub.execute_input":"2024-04-15T19:57:38.632169Z","iopub.status.idle":"2024-04-15T19:57:38.728817Z","shell.execute_reply.started":"2024-04-15T19:57:38.632145Z","shell.execute_reply":"2024-04-15T19:57:38.728075Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Total number of parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(\"Total parameters:\", total_params)\n\n# Total number of trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"Trainable parameters:\", trainable_params)\n\n# Percentage of trainable parameters\ntrainable_percentage = (trainable_params / total_params) * 100\nprint(\"Trainable percentage: {:.2f}%\".format(trainable_percentage))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:41.613991Z","iopub.execute_input":"2024-04-15T19:57:41.614843Z","iopub.status.idle":"2024-04-15T19:57:41.624137Z","shell.execute_reply.started":"2024-04-15T19:57:41.614805Z","shell.execute_reply":"2024-04-15T19:57:41.623060Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Total parameters: 124439808\nTrainable parameters: 124439808\nTrainable percentage: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preparing Training Dataset","metadata":{}},{"cell_type":"code","source":"# load dataset\ndata = load_dataset(\"darkraipro/recipe-instructions\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:46.023638Z","iopub.execute_input":"2024-04-15T19:57:46.024282Z","iopub.status.idle":"2024-04-15T19:57:58.840640Z","shell.execute_reply.started":"2024-04-15T19:57:46.024249Z","shell.execute_reply":"2024-04-15T19:57:58.839644Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/892 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"979b51762fe0454aba4c0257330b40a6"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 150M/150M [00:00<00:00, 191MB/s]  \nDownloading data: 100%|██████████| 150M/150M [00:01<00:00, 137MB/s]  \nDownloading data: 100%|██████████| 15.6M/15.6M [00:00<00:00, 49.1MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/339583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80b71cd37c6943f5a0d0ff3c752814ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/17934 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7f0f429a8446bb9ccae4e4f3fae4db"}},"metadata":{}}]},{"cell_type":"code","source":"def convert_to_tensors(batch):\n    # Convert list of integers in the batch to a tensor, individually for each example\n    batch['input_ids'] = [torch.tensor(e, dtype=torch.long) for e in batch['input_ids']]\n    batch['attention_mask'] = [torch.tensor(e, dtype=torch.long) for e in batch['attention_mask']]\n    return batch\n\n# Note that batched=True applies the function to each batch of examples\ndataset = data.map(convert_to_tensors, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:57:58.842395Z","iopub.execute_input":"2024-04-15T19:57:58.842685Z","iopub.status.idle":"2024-04-15T20:03:17.940319Z","shell.execute_reply.started":"2024-04-15T19:57:58.842660Z","shell.execute_reply":"2024-04-15T20:03:17.939446Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/339583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03edc5012f0e47baa2ecd247c3d0fb15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17934 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aced0a5cb314d5797b221ada7bfea50"}},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef convert_to_tensors(batch):\n    # Convert list of integers in the batch to a single tensor for each\n    batch['input_ids'] = torch.tensor(batch['input_ids'], dtype=torch.long).to(device)\n    batch['attention_mask'] = torch.tensor(batch['attention_mask'], dtype=torch.long).to(device)\n    return batch\n\n# Apply the conversion function to the dataset\n# Note that batched=True applies the function to each batch of examples\ndata.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ndataset = data.map(convert_to_tensors, batched=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Access your dataset in the correct format\ninput_ids = dataset['train']['input_ids']\nattention_mask = dataset['train']['attention_mask']\n\n# Confirm conversion to tensor\nprint(type(input_ids[0]), type(attention_mask[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\n# Initialize data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:07:19.857602Z","iopub.execute_input":"2024-04-15T20:07:19.857908Z","iopub.status.idle":"2024-04-15T20:07:19.879855Z","shell.execute_reply.started":"2024-04-15T20:07:19.857881Z","shell.execute_reply":"2024-04-15T20:07:19.878981Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning Model","metadata":{}},{"cell_type":"code","source":"# Define your training arguments\ntraining_args = TrainingArguments(\n    output_dir='./gpt2-model',\n    num_train_epochs=1,\n    per_device_train_batch_size=4,  # Small batch size\n    gradient_accumulation_steps=8,  # Accumulate gradients to effectively have batch size of 8\n    fp16=True,  # Enable FP16 training\n    gradient_checkpointing=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:07:19.880937Z","iopub.execute_input":"2024-04-15T20:07:19.881218Z","iopub.status.idle":"2024-04-15T20:07:19.888742Z","shell.execute_reply.started":"2024-04-15T20:07:19.881194Z","shell.execute_reply":"2024-04-15T20:07:19.887806Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Initialize Trainer\ntrainer = Trainer(\n    model=GPT2LMHeadModel.from_pretrained(\"gpt2\"),\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset[\"train\"],\n)\n\n# Train model\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n\n# renable warnings\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:07:19.889858Z","iopub.execute_input":"2024-04-15T20:07:19.890152Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240415_200750-kni8o69s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jiatangzhi/huggingface/runs/kni8o69s/workspace' target=\"_blank\">lemon-planet-2</a></strong> to <a href='https://wandb.ai/jiatangzhi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jiatangzhi/huggingface' target=\"_blank\">https://wandb.ai/jiatangzhi/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jiatangzhi/huggingface/runs/kni8o69s/workspace' target=\"_blank\">https://wandb.ai/jiatangzhi/huggingface/runs/kni8o69s/workspace</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1245' max='31836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1245/31836 1:13:01 < 29:57:13, 0.28 it/s, Epoch 0.12/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.478300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.299300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Due to the excessive computational time required, I was compelled to terminate the process.","metadata":{}},{"cell_type":"markdown","source":"# Push model to hub","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n\n# # option 2: key login\n# from huggingface_hub import login\n# write_key = 'hf_' # paste token here\n# login(write_key)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hf_name = 'jiatangzhi' # your hf username or org name\nmodel_id = hf_name + \"/\" + \"gpt2-model\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(model_id)\ntrainer.push_to_hub(model_id)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Fine-tuned Model","metadata":{}},{"cell_type":"code","source":"# load model from hub\nfrom transformers import GPT2LMHeadModel, GPT2Config\nfrom peft import PeftModel, PeftConfig\n\n\nmodel_name = \"gpt2\"\nmodel = GPT2LMHeadModel.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\nconfig = PeftConfig.from_pretrained(\"jiatangzhi/gpt2-model\")\nmodel = PeftModel.from_pretrained(model, \"jiatangzhi/gpt2-model\")\n\n# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Fine-tuned Model","metadata":{}},{"cell_type":"code","source":"intstructions_string = f\"\"\"Can you give me the recipe to make a chocolate cake?\"\"\"\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n\ncomment = \"To make a basic chocolate cake, mix 1 cup sugar, ¾ cup flour, ½ cup cocoa powder, ¾ tsp baking powder, ¾ tsp baking soda, a pinch of salt, 1 egg, ½ cup milk, ¼ cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30-35 minutes. Enjoy your delicious dessert!\"\"\n\nprompt = prompt_template(comment)\nprint(prompt)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\n# inputs = tokenizer(prompt, return_tensors=\"pt\")\n# outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n# print(tokenizer.batch_decode(outputs)[0])\n\n\n# Tokenize the input text using the GPT-2 tokenizer\ninputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True).to(device)\n\n# Generate the output using the GPT-2 model\noutputs = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    max_new_tokens=50  # Specify how many new tokens to generate beyond the input\n)\n\n# Decode the generated output back to a readable string, removing any special tokens\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment = \"Can you give me the recipe to make an benedict eggs?\"\nprompt = prompt_template(comment)\n\nmodel.eval()\n# inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n# print(tokenizer.batch_decode(outputs)[0])\n\n\ninputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True).to(device)\noutputs = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n    max_new_tokens=50  # Specify how many new tokens to generate beyond the input\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{},"execution_count":null,"outputs":[]}]}