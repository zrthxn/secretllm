{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Language Model\n",
    "\n",
    "This time you will build an actual language model, although relatively small, using a dataset of your choosing\n",
    "and we will show you a \"large\" language model. We will also look at various LLM architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a Dataset\n",
    "\n",
    "The first task in building any machine learning application is to find a dataset to work with. There \n",
    "are many online repositories of datasets that you can use for this.\n",
    "\n",
    "The most popular website for NLP related datasets (and models) is [HuggingFace](https://huggingface.co).\n",
    "For this task we recommend that you try to find a dataset in your language or a language you understand.\n",
    "\n",
    "1. Go to https://huggingface.co/datasets\n",
    "2. Under **Main** first filter by *Modalities*, select **Text**. \n",
    "   Under **Tasks**, select **Text Generation**. \n",
    "   Under **Libraries**, select **Datasets**.\n",
    "3. Under **Languages** select your language. This should now give you a list of suitable datasets that we can use.\n",
    "4. Choose any of the datasets you see which is not too big in size and check it's description\n",
    "   for how to load the dataset. This may be different for different datasets. \n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img width=\"30%\" src=\"images/dataset-type.png\">\n",
    "    <img width=\"25%\" src=\"images/dataset-languages.png\">\n",
    "</div>\n",
    "\n",
    "Of course you can choose a dataset according to what you want to build.\n",
    "Other interesting ideas for datasets may be code datasets, math datasets, song lyrics datasets.\n",
    "Finding and getting them ready for use may be more work but would be a good exercise.\n",
    "A good dataset would have at least 100K samples of text, each at least around 100 to 250 words in length. Another important factor is where the data was collected from, for example text from forum websites where people use abbreviations or slang may not be ideal for training a langauge model, if you don't also have samples of good text like news articles or Wikipedia pages.\n",
    "\n",
    "Remember that a machine learning model is only as good as the dataset it uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this Notebook\n",
    "\n",
    "This is the most computationally intensive notebook so far. You will need to provision either a long running Jupyter instance (at least 8-10 hours), \n",
    "or learn to [write a Slurm job](https://doc.zih.tu-dresden.de/jobs_and_resources/slurm/) to run on the HPC, although this may be more difficult for inexperienced users. \n",
    "You can use the [Slurm job file generator](https://doc.zih.tu-dresden.de/jobs_and_resources/slurm_generator/) to write your script once you have figured out how to use the HPC.\n",
    "\n",
    "Here are the values we recommend you should use when creating a Jupyter instance.\n",
    "\n",
    "| Name              | Value               |\n",
    "| ----------------- | ------------------- |\n",
    "| Cluster           | Alpha               |\n",
    "| Generic Resources | gpu:1               |\n",
    "| Runtime           | 12:00:00 (12 hours) |\n",
    "| Project           | p_scads_llm_secrets |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a Dataset\n",
    "\n",
    "Once you find a suitable dataset, you should be able to read the dataset using the `datasets` library.\n",
    "The example in this notebook will use the [Wikipedia dataset](https://huggingface.co/datasets/wikimedia/wikipedia) in Belarusian.\n",
    "This will illustrate how you can use a non-standard character set for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 236165\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "LANG = \"be\"\n",
    "\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", f\"20231101.{LANG}\", cache_dir=\".checkpoints/data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we dont need any of the other columns, we just keep the text column\n",
    "dataset = [ data[\"text\"] for data in dataset[\"train\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Level Tokenization\n",
    "\n",
    "We will build a Byte-Pair Encoding (BPE) tokenizer, but we will use the byte-level version of BPE tokenization.\n",
    "\n",
    "**Byte Level BPE Tokenization** considers a byte to be the smaller unit of text, instead of characters as in simple BPE. \n",
    "This enables it to be applied to more complicated character sets where a character may span more than one byte.\n",
    "It's used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "VOCAB_SIZE = 50_000\n",
    "tokenizer.train_from_iterator(\n",
    "    dataset,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2, \n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tokenization is done at a byte-level, when using UTF-8 encoded characters, the tokens will not remain human readable.\n",
    "Therefore we need to use a byte-level decoder to combine bytes back into UTF-8 encoded characters so that they are human readable again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokens] ['3c733e', 'c390c4b2', 'c394', 'c2a5', 'c391c4a3c391c4a5c390c2b0', 'c390c2b0', 'c4a0c390c2b6', 'c393c4bb', 'c391c4adc391c4a4', 'c393c4bb', '2d', 'c390c2bdc390c2b0c391c4a4', 'c393c4bb', 'c4a0c390c2b0', 'c390c2b0c391c4a7', 'c391c4ad', 'c391c4a3', 'c4a0c390c581', 'c390c2b8c391c4a8', 'c391c4a5c390c2bdc390c2b4c390c2b0', 'c4a0c390c2b0c390c2bc', 'c391c4aac391c4adc390c2bd', 'c4a0c390c2b0', 'c394', 'c2a5', 'c391c4aac390c2b0', 'c392', 'c2b3', 'c393c4bb', 'c390c2b0', 'c4a0c390c4b2c390c2bc', 'c390c2b7c390c2b0c391c4a2c390c2b0', 'c4a0c392', 'c2b3', 'c393c4bb', 'c390c2b0', 'c4a0c390c2b8', 'c390c2b0c391c4aa', 'c391c4ae', 'c392', 'c583', 'c390c2b0c390c2bd', '3c2f733e']\n",
      "[Decode] <s>Аԥсуаа жәытә-натә аахыс Пицунда амшын аԥшаҳәа Амзара ҳәа иашьҭан</s>\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "\n",
    "sentence = \"Аԥсуаа жәытә-натә аахыс Пицунда амшын аԥшаҳәа Амзара ҳәа иашьҭан\"\n",
    "decoder = ByteLevel()\n",
    "\n",
    "sentence = tokenizer.encode(sentence)\n",
    "\n",
    "print(\"[Tokens]\", list(map(lambda x: x.encode(\"utf-8\").hex(), sentence.tokens)))\n",
    "print(\"[Decode]\", decoder.decode(sentence.tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokens are groups of bytes (every 2 hexadecimal characters are one byte) and after decoding this sequence we get back the original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.checkpoints/be-tokenizer/vocab.json',\n",
       " '.checkpoints/be-tokenizer/merges.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(f\".checkpoints/{LANG}-roberta/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our Language Model\n",
    "\n",
    "Language Model is the colloquial term for what the experts call \"causal language modeling\" where the objective of the model is to predict the next word(s) given the last few words.\n",
    "Thus this task is \"causal\" in nature in that each future word is assumed to follow from the past words.\n",
    "\n",
    "The architecture we will use is [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta#roberta). \n",
    "This is a version of [BERT](https://huggingface.co/docs/transformers/v4.46.2/en/model_doc/bert#bert) which is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction. RoBERTa builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(f\".checkpoints/{LANG}-roberta/tokenizer\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForCausalLM\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "model = RobertaForCausalLM(\n",
    "    RobertaConfig(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_position_embeddings=514,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=6,\n",
    "        type_vocab_size=1,\n",
    "        is_decoder=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81966416"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that even such a relatively simple and small model has around 82 million parameters.\n",
    "To put this in perspective, GPT-2 had 1.5 billion parameters and GPT-3 has 175 billion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenizer(dataset, add_special_tokens=True, truncation=True, max_length=512)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=1_000,\n",
    "    weight_decay=0.1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    output_dir=f\".checkpoints/{LANG}-roberta\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_steps=10_000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training! This cell will take a really long time.\n",
    "\n",
    "You can try using different values for the learning rate and number of training epochs to see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\".checkpoints/{LANG}-roberta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Run the model with the text generation pipeline. Pipelines automate the process of tokenization, running the model, and then decoding the output tokens.\n",
    "Here we will take an input prompt in the language that the model was trained in and generate text from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = input(\"Prompt: \")\n",
    "generate = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\")\n",
    "output = generate(prompt, max_new_tokens = 250)[0][\"generated_text\"]\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
