{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider this distribution\n",
    "\n",
    "$FP32: [33.623563422, 12.646104098, 0, -51.5839]$\n",
    "\n",
    "Above tensor is in `fp32` format. Like fp32 other types exist. \n",
    "\n",
    "![data types](./img/data_types.jpg)\n",
    "\n",
    "### General formula to represent values is: $(-1)^{s}\\times 2^{E-127}\\times(1+\\sum_{i=1}^{23}b_{23-i}2^{-i})$ where $b$ is from mantissa [(source)](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Int8` contains 8 bits, represented as integers in memory. The range can be $[0,255]$\n",
    "\n",
    "Converting a 32 bit number to a 8 bit number reduce the space requirement by $4\\times$. However that's not the only benefit. Normally integer arithmetic is much faster and energy efficient\n",
    "\n",
    "### Q: How do you efficiently squeeze all the dynamic range (difference between the highest and the lowest value that can be represented) of `fp32` into `8` bits of `Int8`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider this formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{New Value, } v = \\frac{c - l}{h-l}$\n",
    "Where $c$ is any given value in the old range, $l$ is the lowest value possible in the old range, $h$ is the highest value possible in the old range. \n",
    "\n",
    "## Task 1\n",
    "\n",
    "1. Write code in python that implements this formula. Use the following function given here to generate floating point numbers between a certain range\n",
    "2. Use this formula to squeeze certain values in `fp32` range\n",
    "3. What do you observe? What is the new range of the values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_random_float(min_val: float, max_val: float, size: int = 1, seed: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random floating point numbers between min_val and max_val\n",
    "    \n",
    "    Args:\n",
    "        min_val (float): Minimum value of range\n",
    "        max_val (float): Maximum value of range\n",
    "        size (int): Number of random values to generate\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of random floats\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    random_nums = np.random.uniform(low=min_val, high=max_val, size=size)\n",
    "    return random_nums if size > 1 else random_nums[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "1. Suggest edits to this formula so that we can scale the values in any arbitrary range\n",
    "2. Use it to change the range of certain values in `fp32` range\n",
    "3. What is one problem with this approach (pertaining to numerical representation of the numbers?)\n",
    "4. (Bonus) List more problems with this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are some obvious benefits of running a LLM with fewer precision. \n",
    "\n",
    "1. Less inference time\n",
    "2. Less energy consumption\n",
    "3. Less storage requirement\n",
    "4. Ability to run in mobile devices\n",
    "\n",
    "This comes with the cost of lesser precision and quality of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights vs Activation quantization\n",
    "\n",
    "Weight quantization:\n",
    "\n",
    "1. Store weights in `int8`, dequantize into `fp32` when running it\n",
    "2. Not faster inference, but saves space\n",
    "\n",
    "Activation quantization:\n",
    "\n",
    "1. Convert all inputs and outputs into `int8` and do computations in `int8`\n",
    "2. Need calibration to determine scale factors for data at each layer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero point quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero-point quantization** is a key concept in **quantization**, a technique used to reduce the size and computational requirements of machine learning models by converting higher-precision data (e.g., 32-bit floating-point) into lower-precision formats (e.g., 8-bit integers). Zero-point quantization specifically deals with ensuring that the range of the quantized data matches the original range of the floating-point data.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Zero-Point Quantization?**\n",
    "Quantization typically involves mapping floating-point values to integers. For example, you may map a range of floating-point values (e.g., $[-1.0, 1.0]$) to an integer range (e.g., $[0, 255]$ for 8-bit unsigned integers). However, in many cases, the floating-point range may not start at zero, and we need to account for this offset to avoid information loss during quantization. This is where the **zero point** comes in.\n",
    "\n",
    "The **zero point** is the integer value that corresponds to the real value of **0** in the original floating-point range.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Representation**\n",
    "\n",
    "Given:\n",
    "1. A floating-point value $x$,\n",
    "2. The quantization scale $s$ (i.e., the step size for mapping floating-point values to integers),\n",
    "3. The zero-point $z$ (the offset),\n",
    "\n",
    "The quantized integer value $q$ is calculated as:\n",
    "$$\n",
    "q = \\text{round} \\left( \\frac{x}{s} \\right) + z\n",
    "$$\n",
    "\n",
    "The dequantized value (to recover the floating-point value) is:\n",
    "$$\n",
    "x \\approx s \\cdot (q - z)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **How Zero-Point Works**\n",
    "\n",
    "1. **Scale and Zero-Point Computation:**\n",
    "   - The scale ($s$) determines how much precision is preserved during quantization.\n",
    "   - The zero point ($z$) ensures that the range of the quantized values aligns with the original data.\n",
    "\n",
    "2. **Mapping Floating-Point Range to Integer Range:**\n",
    "   - For example, if the floating-point range is $[-1.0, 1.0]$ and the integer range is $[0, 255]$, the scale would be:\n",
    "     $$\n",
    "     s = \\frac{\\text{max} - \\text{min}}{\\text{int\\_max} - \\text{int\\_min}} = \\frac{1.0 - (-1.0)}{255 - 0} = 0.007843\n",
    "     $$\n",
    "   - The zero-point $z$ ensures that $x = 0.0$ maps to an integer within the range. For unsigned integers, $z$ would be:\n",
    "     $$\n",
    "     z = \\text{round} \\left( \\frac{0.0 - \\text{min}}{s} \\right) = \\text{round} \\left( \\frac{0.0 - (-1.0)}{0.007843} \\right) = 128\n",
    "     $$\n",
    "\n",
    "3. **Key Observation:**\n",
    "   - If the range of the floating-point data is symmetric around 0 (e.g., $[-1.0, 1.0]$), the zero point will often fall in the middle of the integer range.\n",
    "   - If the range is not symmetric (e.g., $[0.0, 1.0]$), the zero point will adjust accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Zero-Point Important?**\n",
    "- **Alignment of Ranges:** Ensures that quantized values accurately represent the original floating-point values, especially for zero.\n",
    "- **Avoid Information Loss:** Without a zero point, quantization could introduce a significant bias by shifting the representation of zero, leading to errors during dequantization.\n",
    "- **Improved Model Performance:** Proper zero-point computation minimizes the error introduced by quantization, preserving model accuracy while reducing memory and computation requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Quantization with Zero Point**\n",
    "\n",
    "#### Floating-Point Range: $[-6, 6]$, Integer Range: $[0, 255]$\n",
    "1. Compute the scale:\n",
    "   $$\n",
    "   s = \\frac{\\text{max} - \\text{min}}{\\text{int\\_max} - \\text{int\\_min}} = \\frac{6 - (-6)}{255 - 0} = \\frac{12}{255} \\approx 0.047\n",
    "   $$\n",
    "\n",
    "2. Compute the zero point:\n",
    "   $$\n",
    "   z = \\text{round} \\left( \\frac{0 - \\text{min}}{s} \\right) = \\text{round} \\left( \\frac{0 - (-6)}{0.047} \\right) = \\text{round}(127.66) \\approx 128\n",
    "   $$\n",
    "\n",
    "3. Quantize a floating-point value $x = 3.5$:\n",
    "   $$\n",
    "   q = \\text{round} \\left( \\frac{x}{s} \\right) + z = \\text{round} \\left( \\frac{3.5}{0.047} \\right) + 128 = \\text{round}(74.47) + 128 = 202\n",
    "   $$\n",
    "\n",
    "4. Dequantize back to floating-point:\n",
    "   $$\n",
    "   x \\approx s \\cdot (q - z) = 0.047 \\cdot (202 - 128) = 0.047 \\cdot 74 \\approx 3.48\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "- Zero-point quantization adjusts the mapping of floating-point numbers to integers to ensure that $0.0$ is accurately represented.\n",
    "- It prevents bias and improves accuracy when converting between floating-point and quantized representations.\n",
    "- Common in 8-bit quantization for neural networks to optimize memory and computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "1. Solve 0 point quantization for floating point range $[-10, 10]$ and integer range $[-127, 127]$ and with the help of one example, quantize and dequantize a value. What's the issue?\n",
    "2. Write python code for calculating 0 point quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTQ quantization for 4 bit integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian matrix\n",
    "\n",
    "The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. Mathematically, for a function $ f(x_1, x_2, \\ldots, x_n) $, the Hessian is defined as:\n",
    "\n",
    "$$\n",
    "H(f) = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Key Properties:**\n",
    "- It is symmetric if $ f $ is twice continuously differentiable.\n",
    "- The eigenvalues of the Hessian give insight into the local curvature of $ f $:\n",
    "  - Positive eigenvalues: $ f $ is convex (local minimum).\n",
    "  - Negative eigenvalues: $ f $ is concave (local maximum).\n",
    "  - Mixed signs: $ f $ has a saddle point.\n",
    "\n",
    "It tells us how the loss is updating. What is the error in our network as we are updating our weights, which allows us to minimize our loss. How our loss is effected by updating the weights.\n",
    "\n",
    "### **The Hessian matrix contains the second order derivative of the loss function with respect to the weights. The inverse of this matrix quantifies how changes in the parameters affect the loss function**\n",
    "\n",
    "Magnitude of the diagonal elements of the inverted Hessian matrix tells us which weights are more sensitive to quantization and which weights are less sensitive to quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "The primary objective is to convert the network's parameters (weights) into 4-bit integers (values between -8 and 7, or 0 and 15 depending on the quantization scheme), except for \"emergent features\" which refer to specific parts of the network that are deemed crucial and kept in higher precision. The process aims to minimize the loss introduced by this quantization and find the best possible quantized values for the parameters.\n",
    "\n",
    "1. We need to minimize the loss function from the *quantization*\n",
    "2. We need to know how to compute the best parameter values for this quantization\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "The process is performed layer by layer:\n",
    "\n",
    "1. Layer-wise Quantization: The weights of each layer are processed independently.\n",
    "\n",
    "2. Find the Hessian Matrix: Compute the Hessian matrix for the current layer.\n",
    "\n",
    "3. Apply a Damping Factor: A damping factor is applied to the Hessian matrix. This is a regularization technique to prevent the Hessian from overfitting to the current training data, which could lead to poor generalization after quantization. (Just like gradient descent, hessian can also overfit)\n",
    "\n",
    "4. Find the Inverse Hessian: Calculate the inverse of the damped Hessian matrix. The inverse Hessian would allow us to see the loss *sensitivity* of weights, as we are updating the weights.\n",
    "    - Higher values are less sensitive\n",
    "    - Lower values are more sensitive\n",
    "\n",
    "5. Process the Weights: For each weight in the layer:\n",
    "    - Perform Zero-Point Quantization: Quantize the weight to a 4-bit integer using a zero-point quantization scheme.\n",
    "    - Calculate the Quantization Error: Determine the difference between the original floating-point weight and its quantized value.\n",
    "    - Normalize the Quantization Error: Normalize the quantization error by the corresponding diagonal element of the inverse Hessian. This scales the error based on the weight's sensitivity to changes in the loss. Weights with higher sensitivity will have their quantization errors adjusted more significantly.\n",
    "    - Update Remaining Weight Vectors: Update the other weights in the layer based on the normalized quantization error and the inverse Hessian. This step aims to compensate for the quantization error of one weight by making small adjustments to other weights, minimizing the overall impact on the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python code to implement GPTQ quantization for a mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import os\n",
    "\n",
    "def quantize_and_save_model():\n",
    "    # Model ID for Qwen 0.5B\n",
    "    model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "    \n",
    "    # Output directory for the quantized model\n",
    "    output_dir = \"quantized_qwen\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # Load model in FP16 for faster quantization\n",
    "    print(\"Loading model in FP16...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Prepare quantization config\n",
    "    quantization_config = BaseQuantizeConfig(\n",
    "        bits=4,  # Quantize to 4 bits\n",
    "        group_size=128,  # Standard group size for GPTQ\n",
    "        desc_act=True,  # Enable activation description\n",
    "    )\n",
    "\n",
    "    # Initialize GPTQ model\n",
    "    print(\"Initializing GPTQ model...\")\n",
    "    gptq_model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        model,\n",
    "        quantization_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Prepare calibration dataset\n",
    "    # Using a small sample of text for calibration\n",
    "    examples = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Machine learning is a subset of artificial intelligence.\",\n",
    "        \"Python is a versatile programming language.\",\n",
    "    ]\n",
    "    \n",
    "    # Tokenize calibration data\n",
    "    calibration_data = tokenizer(\n",
    "        examples,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Run quantization\n",
    "    print(\"Starting quantization...\")\n",
    "    gptq_model.quantize(calibration_data[\"input_ids\"])\n",
    "\n",
    "    # Save quantized model\n",
    "    print(\"Saving quantized model...\")\n",
    "    gptq_model.save_quantized(output_dir)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"Quantization complete! Model saved to: {output_dir}\")\n",
    "    return output_dir\n",
    " \n",
    "output_dir = quantize_and_save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
