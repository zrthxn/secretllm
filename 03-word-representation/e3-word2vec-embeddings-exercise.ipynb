{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E3 - Word Representations\n",
    "### 1. Training a Word Embedder using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting a Dataset\n",
    "Find a good dataset in any language you like. It does not have to be English, any language you understand well or know well enough to judge a model will work. \n",
    "\n",
    "We will be exploring different Word Embeddings and Tokenizers. \n",
    "Here are some resources for datasets to use for Tokenization:\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "\n",
    "- [Hugging Face Datasets](https://huggingface.co/datasets)\n",
    "\n",
    "- [The Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download)\n",
    "\n",
    "- [The Natural Tokenization Processing](https://www.nltk.org/nltk_data)\n",
    "\n",
    "- [Common Crawl](https://commoncrawl.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk gensim scikit-learn matplotlib gensim torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: download the pre-trained nltk 'punkt' word tokenizer\n",
    "\n",
    "# TODO: Load your chosen dataset, e. g. from the Leipzig Corpora Collection\n",
    "with open('your_dataset', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# TODO: Perform basic preprocessing (lowercasing, removing special characters) with regular expressions\n",
    "\n",
    "# TODO: Use the word_tokenize() function to tokenize the text and print the first ten tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings with Word2Vec\n",
    "\n",
    "We use Word2Vec to create word embeddings. Word2Vec learns vector representations of words based on the context in which they appear, allowing similar words to have closer vectors in the embedding space.\n",
    "Word2Vec has two main approaches:\n",
    "1. **Skip-Gram**: Predicts context words from a center word.\n",
    "2. **Continuous Bag of Words (CBOW)**: Predicts a center word from surrounding context words.\n",
    "For simplicity, we'll use the CBOW approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 \n",
    "window_size = 5\n",
    "min_word_count = 1\n",
    "workers = 4\n",
    "\n",
    "# TODO: Train Word2Vec model with 'Word2Vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore and Visualize the Word Embeddings\n",
    "\n",
    "Once the model is trained, we can explore the word embeddings. We'll look at some examples of similar words and visualize the embedding space with dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select a subset of words to visualize and play around with different tokens. \n",
    "words_to_visualize = ['hello'] # Play around with different words, but they need to be included in the vocab\n",
    "word_vectors = [word2vec_model.wv[word] for word in words_to_visualize]\n",
    "\n",
    "# Reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the word vectors\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, word in enumerate(words_to_visualize):\n",
    "    plt.scatter(reduced_vectors[i][0], reduced_vectors[i][1])\n",
    "    plt.annotate(word, xy=(reduced_vectors[i][0], reduced_vectors[i][1]), fontsize=12)\n",
    "plt.title(\"2D Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load\n",
    "dataset = load(\"text8\")\n",
    "sentences = list(dataset)  # Each sentence as a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=sentences, vector_size=embedding_dim, window=window_size,\n",
    "                          min_count=min_word_count, workers=workers)\n",
    "print(\"Word2Vec model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction and Visualization with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Try different words with the trained word2vec model, trained on text8 dataset\n",
    "words_to_visualize = ['king', 'queen', 'man', 'woman', 'great', 'awesome', 'amazing']\n",
    "word_vectors = [model.wv[word] for word in words_to_visualize if word in model.wv]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, word in enumerate(words_to_visualize):\n",
    "    plt.scatter(reduced_vectors[i][0], reduced_vectors[i][1])\n",
    "    plt.annotate(word, xy=(reduced_vectors[i][0], reduced_vectors[i][1]), fontsize=12)\n",
    "plt.title(\"2D Visualization of Word Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch v2.x (Machine Learning)",
   "language": "python",
   "name": "pytorch_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
