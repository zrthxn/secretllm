{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization is at the heart of much weirdness of LLMs. \n",
    "\n",
    "You can play around with different kind of tokenizers here in [this website](https://tiktokenizer.vercel.app/).\n",
    "Check out how code an foreign languages are represented in different tokenizers. Check how spaces in python are handled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strings** are just numbers encoded by a standard text encoding scheme like unicode. To get the unicode number representation for a number we can use the `ord` function in python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l: 108\n",
      "ü´®: 129768\n",
      "[104, 105, 32, 116, 104, 101, 114, 101, 32, 128075, 127996]\n"
     ]
    }
   ],
   "source": [
    "print(\"l: \" + str(ord(\"l\")))\n",
    "print(\"ü´®: \" + str(ord(\"ü´®\")))\n",
    "print([ord(c) for c in \"hi there üëãüèº\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode supports about 150k such numbers. Adding support to all the different characters supported by Unicode will massively increase the input to our language model. Standards can also change. We primarily use unicode (utf-8) encoding because it's fairly concise in terms of the size of the representation for english and it's backwards compatible with ASCII. This would also count as a character tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before implementing BPE tokenization, we want to start with simple whitespace tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with simple whitespace tokenization: ['ScaDS.AI', '(Center', 'for', 'Scalable', 'Data', 'Analytics', 'and', 'Artificial', 'Intelligence)', 'Dresden/Leipzig', 'is', 'a', 'center', 'for', 'Data', 'Science,', 'Artificial', 'Intelligence', 'and', 'Big', 'Data', 'with', 'locations', 'in', 'Dresden', 'and', 'Leipzig.', 'One', 'of', 'five', 'new', 'AI', 'centers', 'in', 'Germany', 'funded', 'under', 'the', 'federal', 'government‚Äôs', 'AI', 'strategy', 'by', 'the', 'Federal', 'Ministry', 'of', 'Education', 'and', 'Research', '(BMBF)', 'and', 'the', 'Free', 'State', 'of', 'Saxony.', 'Established', 'as', 'a', 'permanent', 'research', 'facility', 'at', 'both', 'locations', 'with', 'strong', 'connections', 'to', 'the', 'local', 'universities:', 'TU', 'Dresden', 'and', 'Leipzig', 'University.', 'Over', '60', 'Principal', 'Investigators,', 'more', 'than', '180', 'employees', 'and', 'up', 'to', '12', 'new', 'AI', 'professorships', 'in', 'Dresden', 'and', 'Leipzig']\n",
      "Tokens after preprocessing: ['scadsai', 'center', 'for', 'scalable', 'data', 'analytics', 'and', 'artificial', 'intelligence', 'dresdenleipzig', 'is', 'a', 'center', 'for', 'data', 'science', 'artificial', 'intelligence', 'and', 'big', 'data', 'with', 'locations', 'in', 'dresden', 'and', 'leipzig', 'one', 'of', 'five', 'new', 'ai', 'centers', 'in', 'germany', 'funded', 'under', 'the', 'federal', 'governments', 'ai', 'strategy', 'by', 'the', 'federal', 'ministry', 'of', 'education', 'and', 'research', 'bmbf', 'and', 'the', 'free', 'state', 'of', 'saxony', 'established', 'as', 'a', 'permanent', 'research', 'facility', 'at', 'both', 'locations', 'with', 'strong', 'connections', 'to', 'the', 'local', 'universities', 'tu', 'dresden', 'and', 'leipzig', 'university', 'over', '60', 'principal', 'investigators', 'more', 'than', '180', 'employees', 'and', 'up', 'to', '12', 'new', 'ai', 'professorships', 'in', 'dresden', 'and', 'leipzig']\n",
      "Top 10 unigrams and their frequencies: [('and', 8), ('the', 4), ('data', 3), ('in', 3), ('dresden', 3), ('leipzig', 3), ('of', 3), ('ai', 3), ('center', 2), ('for', 2)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "text = \"\"\"ScaDS.AI (Center for Scalable Data Analytics and Artificial Intelligence) Dresden/Leipzig is a center for Data Science, Artificial Intelligence and Big Data with locations in Dresden and Leipzig. One of five new AI centers in Germany funded under the federal government‚Äôs AI strategy by the Federal Ministry of Education and Research (BMBF) and the Free State of Saxony. Established as a permanent research facility at both locations with strong connections to the local universities: TU Dresden and Leipzig University. Over 60 Principal Investigators, more than 180 employees and up to 12 new AI professorships in Dresden and Leipzig\"\"\"\n",
    "\n",
    "# TODO: Split the text by whitespace\n",
    "word_tokens = text.split()\n",
    "print(\"Tokens with simple whitespace tokenization:\", word_tokens)\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    # TODO: Convert into lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Keep alphanumeric and whitespace only\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "word_tokens = simple_tokenizer(text)\n",
    "print(\"Tokens after preprocessing:\", word_tokens)\n",
    "\n",
    "# TODO: Calculate unigram frequencies - hint: you can use the Counter Class\n",
    "unigram_freq = Counter(word_tokens)\n",
    "print(\"Top 10 unigrams and their frequencies:\", unigram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword-Level-Tokenization - Byte Pair Encoding (BPE) algorithm\n",
    "\n",
    "Feeding raw UTF-8 would be really nice, but the downside to that is the long context length. Remember our model has a limited amount of memory, thus we need to compress the raw text input into something smaller (ideally variable length) - which still retains the same information as our original text. Also, splitting text by whitespaces is very easy, but it does not work well with rare, Out-Of-Bag (OOB) words. \n",
    "\n",
    "The classic idea of text compression (Huffman Coding) says we put more effort (more characters, more memory) to represent characters which are rare in our sequence and characters or rather sequence of characters which repeat a lot can be represented with shorter symbols or less memory\n",
    "\n",
    "The BPE algorithm follows from the same idea. For example\n",
    "Suppose we have the following string:\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a bute that is not used in the data, such as \"Z\". Now there is the following data and replacement table\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "Then this process is repeated. We keep minting new tokens (symbols) to replace old symbols which repeat frequently\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "(Much like how we expand grammar in formal languages)\n",
    "\n",
    "And finally\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "**Vocabulary** refers to the number of unique symbols we use to represent our text. For decimal number system with base 10 the vocabulary is 10 (0-9) and for english the vocabulary of characters is 26 (a-z). Note that by replacing `Z=aa` we effectively reduced the length of our string but the vocabulary size has increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 54\n",
      "length of text: 634\n",
      "Tokens from UTF-8 Values:\n",
      "[83, 99, 97, 68, 83, 46, 65, 73, 32, 40, 67, 101, 110, 116, 101, 114, 32, 102, 111, 114, 32, 83, 99, 97, 108, 97, 98, 108, 101, 32, 68, 97, 116, 97, 32, 65, 110, 97, 108, 121, 116, 105, 99, 115, 32, 97, 110, 100, 32, 65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 73, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101, 41, 32, 68, 114, 101, 115, 100, 101, 110, 47, 76, 101, 105, 112, 122, 105, 103, 32, 105, 115, 32, 97, 32, 99, 101, 110, 116, 101, 114, 32, 102, 111, 114, 32, 68, 97, 116, 97, 32, 83, 99, 105, 101, 110, 99, 101, 44, 32, 65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 73, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101, 32, 97, 110, 100, 32, 66, 105, 103, 32, 68, 97, 116, 97, 32, 119, 105, 116, 104, 32, 108, 111, 99, 97, 116, 105, 111, 110, 115, 32, 105, 110, 32, 68, 114, 101, 115, 100, 101, 110, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103, 46, 32, 79, 110, 101, 32, 111, 102, 32, 102, 105, 118, 101, 32, 110, 101, 119, 32, 65, 73, 32, 99, 101, 110, 116, 101, 114, 115, 32, 105, 110, 32, 71, 101, 114, 109, 97, 110, 121, 32, 102, 117, 110, 100, 101, 100, 32, 117, 110, 100, 101, 114, 32, 116, 104, 101, 32, 102, 101, 100, 101, 114, 97, 108, 32, 103, 111, 118, 101, 114, 110, 109, 101, 110, 116, 226, 128, 153, 115, 32, 65, 73, 32, 115, 116, 114, 97, 116, 101, 103, 121, 32, 98, 121, 32, 116, 104, 101, 32, 70, 101, 100, 101, 114, 97, 108, 32, 77, 105, 110, 105, 115, 116, 114, 121, 32, 111, 102, 32, 69, 100, 117, 99, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 82, 101, 115, 101, 97, 114, 99, 104, 32, 40, 66, 77, 66, 70, 41, 32, 97, 110, 100, 32, 116, 104, 101, 32, 70, 114, 101, 101, 32, 83, 116, 97, 116, 101, 32, 111, 102, 32, 83, 97, 120, 111, 110, 121, 46, 32, 69, 115, 116, 97, 98, 108, 105, 115, 104, 101, 100, 32, 97, 115, 32, 97, 32, 112, 101, 114, 109, 97, 110, 101, 110, 116, 32, 114, 101, 115, 101, 97, 114, 99, 104, 32, 102, 97, 99, 105, 108, 105, 116, 121, 32, 97, 116, 32, 98, 111, 116, 104, 32, 108, 111, 99, 97, 116, 105, 111, 110, 115, 32, 119, 105, 116, 104, 32, 115, 116, 114, 111, 110, 103, 32, 99, 111, 110, 110, 101, 99, 116, 105, 111, 110, 115, 32, 116, 111, 32, 116, 104, 101, 32, 108, 111, 99, 97, 108, 32, 117, 110, 105, 118, 101, 114, 115, 105, 116, 105, 101, 115, 58, 32, 84, 85, 32, 68, 114, 101, 115, 100, 101, 110, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103, 32, 85, 110, 105, 118, 101, 114, 115, 105, 116, 121, 46, 32, 79, 118, 101, 114, 32, 54, 48, 32, 80, 114, 105, 110, 99, 105, 112, 97, 108, 32, 73, 110, 118, 101, 115, 116, 105, 103, 97, 116, 111, 114, 115, 44, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 49, 56, 48, 32, 101, 109, 112, 108, 111, 121, 101, 101, 115, 32, 97, 110, 100, 32, 117, 112, 32, 116, 111, 32, 49, 50, 32, 110, 101, 119, 32, 65, 73, 32, 112, 114, 111, 102, 101, 115, 115, 111, 114, 115, 104, 105, 112, 115, 32, 105, 110, 32, 68, 114, 101, 115, 100, 101, 110, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103]\n",
      "length of tokens: 636\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"ScaDS.AI (Center for Scalable Data Analytics and Artificial Intelligence) Dresden/Leipzig is a center for Data Science, Artificial Intelligence and Big Data with locations in Dresden and Leipzig. One of five new AI centers in Germany funded under the federal government‚Äôs AI strategy by the Federal Ministry of Education and Research (BMBF) and the Free State of Saxony. Established as a permanent research facility at both locations with strong connections to the local universities: TU Dresden and Leipzig University. Over 60 Principal Investigators, more than 180 employees and up to 12 new AI professorships in Dresden and Leipzig\"\"\"\n",
    "\n",
    "bytes = text.encode('utf-8')\n",
    "tokens = [int(token) for token in bytes]\n",
    "vocab_size = len(set(tokens))\n",
    "print(\"Vocabulary Size: \" + str(vocab_size))\n",
    "mapping = {token: chr(token) for token in tokens}\n",
    "print(\"length of text:\", len(text))\n",
    "print(\"Tokens from UTF-8 Values:\")\n",
    "print(tokens)\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to count which pairs of characters are occuring the most amount of time. So we write a function for the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(83, 99): 3, (99, 97): 6, (97, 68): 1, (68, 83): 1, (83, 46): 1, (46, 65): 1, (65, 73): 4, (73, 32): 4, (32, 40): 2, (40, 67): 1, (67, 101): 1, (101, 110): 12, (110, 116): 7, (116, 101): 7, (101, 114): 12, (114, 32): 6, (32, 102): 6, (102, 111): 2, (111, 114): 5, (32, 83): 4, (97, 108): 8, (108, 97): 1, (97, 98): 2, (98, 108): 2, (108, 101): 1, (101, 32): 11, (32, 68): 7, (68, 97): 3, (97, 116): 10, (116, 97): 5, (97, 32): 5, (32, 65): 6, (65, 110): 1, (110, 97): 1, (108, 121): 1, (121, 116): 1, (116, 105): 9, (105, 99): 3, (99, 115): 1, (115, 32): 10, (32, 97): 12, (97, 110): 11, (110, 100): 10, (100, 32): 10, (65, 114): 2, (114, 116): 2, (105, 102): 2, (102, 105): 3, (99, 105): 5, (105, 97): 2, (108, 32): 6, (32, 73): 3, (73, 110): 3, (101, 108): 2, (108, 108): 2, (108, 105): 4, (105, 103): 8, (103, 101): 2, (110, 99): 4, (99, 101): 5, (101, 41): 1, (41, 32): 2, (68, 114): 4, (114, 101): 7, (101, 115): 10, (115, 100): 4, (100, 101): 8, (110, 47): 1, (47, 76): 1, (76, 101): 4, (101, 105): 4, (105, 112): 6, (112, 122): 4, (122, 105): 4, (103, 32): 4, (32, 105): 4, (105, 115): 3, (32, 99): 3, (105, 101): 2, (101, 44): 1, (44, 32): 2, (32, 66): 1, (66, 105): 1, (32, 119): 2, (119, 105): 2, (105, 116): 5, (116, 104): 8, (104, 32): 5, (32, 108): 3, (108, 111): 4, (111, 99): 3, (105, 111): 4, (111, 110): 7, (110, 115): 3, (105, 110): 5, (110, 32): 8, (32, 76): 3, (103, 46): 1, (46, 32): 3, (32, 79): 2, (79, 110): 1, (110, 101): 5, (32, 111): 3, (111, 102): 4, (102, 32): 3, (105, 118): 3, (118, 101): 6, (32, 110): 2, (101, 119): 2, (119, 32): 2, (114, 115): 5, (32, 71): 1, (71, 101): 1, (114, 109): 2, (109, 97): 2, (110, 121): 2, (121, 32): 5, (102, 117): 1, (117, 110): 3, (101, 100): 4, (32, 117): 3, (32, 116): 7, (104, 101): 5, (102, 101): 2, (114, 97): 3, (32, 103): 1, (103, 111): 1, (111, 118): 1, (114, 110): 1, (110, 109): 1, (109, 101): 1, (116, 226): 1, (226, 128): 1, (128, 153): 1, (153, 115): 1, (32, 115): 2, (115, 116): 5, (116, 114): 3, (101, 103): 1, (103, 121): 1, (32, 98): 2, (98, 121): 1, (32, 70): 2, (70, 101): 1, (32, 77): 1, (77, 105): 1, (110, 105): 3, (114, 121): 1, (32, 69): 2, (69, 100): 1, (100, 117): 1, (117, 99): 1, (32, 82): 1, (82, 101): 1, (115, 101): 2, (101, 97): 2, (97, 114): 2, (114, 99): 2, (99, 104): 2, (40, 66): 1, (66, 77): 1, (77, 66): 1, (66, 70): 1, (70, 41): 1, (70, 114): 1, (101, 101): 2, (83, 116): 1, (83, 97): 1, (97, 120): 1, (120, 111): 1, (121, 46): 2, (69, 115): 1, (115, 104): 2, (97, 115): 1, (32, 112): 2, (112, 101): 1, (116, 32): 2, (32, 114): 1, (102, 97): 1, (97, 99): 1, (105, 108): 1, (116, 121): 2, (98, 111): 1, (111, 116): 1, (114, 111): 2, (110, 103): 1, (99, 111): 1, (110, 110): 1, (101, 99): 1, (99, 116): 1, (116, 111): 3, (111, 32): 2, (115, 105): 2, (115, 58): 1, (58, 32): 1, (32, 84): 1, (84, 85): 1, (85, 32): 1, (32, 85): 1, (85, 110): 1, (79, 118): 1, (32, 54): 1, (54, 48): 1, (48, 32): 2, (32, 80): 1, (80, 114): 1, (114, 105): 1, (112, 97): 1, (110, 118): 1, (103, 97): 1, (115, 44): 1, (32, 109): 1, (109, 111): 1, (104, 97): 1, (32, 49): 2, (49, 56): 1, (56, 48): 1, (32, 101): 1, (101, 109): 1, (109, 112): 1, (112, 108): 1, (111, 121): 1, (121, 101): 1, (117, 112): 1, (112, 32): 1, (49, 50): 1, (50, 32): 1, (112, 114): 1, (115, 115): 1, (115, 111): 1, (104, 105): 1, (112, 115): 1}\n",
      "[(12, (101, 114)), (12, (101, 110)), (12, (32, 97)), (11, (101, 32)), (11, (97, 110)), (10, (115, 32)), (10, (110, 100)), (10, (101, 115)), (10, (100, 32)), (10, (97, 116)), (9, (116, 105)), (8, (116, 104)), (8, (110, 32)), (8, (105, 103)), (8, (100, 101)), (8, (97, 108)), (7, (116, 101)), (7, (114, 101)), (7, (111, 110)), (7, (110, 116)), (7, (32, 116)), (7, (32, 68)), (6, (118, 101)), (6, (114, 32)), (6, (108, 32)), (6, (105, 112)), (6, (99, 97)), (6, (32, 102)), (6, (32, 65)), (5, (121, 32)), (5, (116, 97)), (5, (115, 116)), (5, (114, 115)), (5, (111, 114)), (5, (110, 101)), (5, (105, 116)), (5, (105, 110)), (5, (104, 101)), (5, (104, 32)), (5, (99, 105)), (5, (99, 101)), (5, (97, 32)), (4, (122, 105)), (4, (115, 100)), (4, (112, 122)), (4, (111, 102)), (4, (110, 99)), (4, (108, 111)), (4, (108, 105)), (4, (105, 111)), (4, (103, 32)), (4, (101, 105)), (4, (101, 100)), (4, (76, 101)), (4, (73, 32)), (4, (68, 114)), (4, (65, 73)), (4, (32, 105)), (4, (32, 83)), (3, (117, 110)), (3, (116, 114)), (3, (116, 111)), (3, (114, 97)), (3, (111, 99)), (3, (110, 115)), (3, (110, 105)), (3, (105, 118)), (3, (105, 115)), (3, (105, 99)), (3, (102, 105)), (3, (102, 32)), (3, (83, 99)), (3, (73, 110)), (3, (68, 97)), (3, (46, 32)), (3, (32, 117)), (3, (32, 111)), (3, (32, 108)), (3, (32, 99)), (3, (32, 76)), (3, (32, 73)), (2, (121, 46)), (2, (119, 105)), (2, (119, 32)), (2, (116, 121)), (2, (116, 32)), (2, (115, 105)), (2, (115, 104)), (2, (115, 101)), (2, (114, 116)), (2, (114, 111)), (2, (114, 109)), (2, (114, 99)), (2, (111, 32)), (2, (110, 121)), (2, (109, 97)), (2, (108, 108)), (2, (105, 102)), (2, (105, 101)), (2, (105, 97)), (2, (103, 101)), (2, (102, 111)), (2, (102, 101)), (2, (101, 119)), (2, (101, 108)), (2, (101, 101)), (2, (101, 97)), (2, (99, 104)), (2, (98, 108)), (2, (97, 114)), (2, (97, 98)), (2, (65, 114)), (2, (48, 32)), (2, (44, 32)), (2, (41, 32)), (2, (32, 119)), (2, (32, 115)), (2, (32, 112)), (2, (32, 110)), (2, (32, 98)), (2, (32, 79)), (2, (32, 70)), (2, (32, 69)), (2, (32, 49)), (2, (32, 40)), (1, (226, 128)), (1, (153, 115)), (1, (128, 153)), (1, (121, 116)), (1, (121, 101)), (1, (120, 111)), (1, (117, 112)), (1, (117, 99)), (1, (116, 226)), (1, (115, 115)), (1, (115, 111)), (1, (115, 58)), (1, (115, 44)), (1, (114, 121)), (1, (114, 110)), (1, (114, 105)), (1, (112, 115)), (1, (112, 114)), (1, (112, 108)), (1, (112, 101)), (1, (112, 97)), (1, (112, 32)), (1, (111, 121)), (1, (111, 118)), (1, (111, 116)), (1, (110, 118)), (1, (110, 110)), (1, (110, 109)), (1, (110, 103)), (1, (110, 97)), (1, (110, 47)), (1, (109, 112)), (1, (109, 111)), (1, (109, 101)), (1, (108, 121)), (1, (108, 101)), (1, (108, 97)), (1, (105, 108)), (1, (104, 105)), (1, (104, 97)), (1, (103, 121)), (1, (103, 111)), (1, (103, 97)), (1, (103, 46)), (1, (102, 117)), (1, (102, 97)), (1, (101, 109)), (1, (101, 103)), (1, (101, 99)), (1, (101, 44)), (1, (101, 41)), (1, (100, 117)), (1, (99, 116)), (1, (99, 115)), (1, (99, 111)), (1, (98, 121)), (1, (98, 111)), (1, (97, 120)), (1, (97, 115)), (1, (97, 99)), (1, (97, 68)), (1, (85, 110)), (1, (85, 32)), (1, (84, 85)), (1, (83, 116)), (1, (83, 97)), (1, (83, 46)), (1, (82, 101)), (1, (80, 114)), (1, (79, 118)), (1, (79, 110)), (1, (77, 105)), (1, (77, 66)), (1, (71, 101)), (1, (70, 114)), (1, (70, 101)), (1, (70, 41)), (1, (69, 115)), (1, (69, 100)), (1, (68, 83)), (1, (67, 101)), (1, (66, 105)), (1, (66, 77)), (1, (66, 70)), (1, (65, 110)), (1, (58, 32)), (1, (56, 48)), (1, (54, 48)), (1, (50, 32)), (1, (49, 56)), (1, (49, 50)), (1, (47, 76)), (1, (46, 65)), (1, (40, 67)), (1, (40, 66)), (1, (32, 114)), (1, (32, 109)), (1, (32, 103)), (1, (32, 101)), (1, (32, 85)), (1, (32, 84)), (1, (32, 82)), (1, (32, 80)), (1, (32, 77)), (1, (32, 71)), (1, (32, 66)), (1, (32, 54))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    # TODO: Implement the function body. It should return a dictionary with each pair as key and the frequency as its value. \n",
    "    counts = {} # count of each id\n",
    "    for pair in zip(ids, ids[1:]): # iterate over all pairs \n",
    "        counts[pair] = counts.get(pair, 0) + 1 # if pair is not in counts, return 0 otherwise increment by 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "print(sorted(((v, k) for k,v in stats.items()), reverse=True)) # sort by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 110)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to merge this pair so we write a merge function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 999, 1, 5]\n",
      "[83, 99, 97, 68, 83, 46, 65, 73, 32, 40, 67, 637, 116, 101, 114, 32, 102, 111, 114, 32, 83, 99, 97, 108, 97, 98, 108, 101, 32, 68, 97, 116, 97, 32, 65, 110, 97, 108, 121, 116, 105, 99, 115, 32, 97, 110, 100, 32, 65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 73, 110, 116, 101, 108, 108, 105, 103, 637, 99, 101, 41, 32, 68, 114, 101, 115, 100, 637, 47, 76, 101, 105, 112, 122, 105, 103, 32, 105, 115, 32, 97, 32, 99, 637, 116, 101, 114, 32, 102, 111, 114, 32, 68, 97, 116, 97, 32, 83, 99, 105, 637, 99, 101, 44, 32, 65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 73, 110, 116, 101, 108, 108, 105, 103, 637, 99, 101, 32, 97, 110, 100, 32, 66, 105, 103, 32, 68, 97, 116, 97, 32, 119, 105, 116, 104, 32, 108, 111, 99, 97, 116, 105, 111, 110, 115, 32, 105, 110, 32, 68, 114, 101, 115, 100, 637, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103, 46, 32, 79, 110, 101, 32, 111, 102, 32, 102, 105, 118, 101, 32, 110, 101, 119, 32, 65, 73, 32, 99, 637, 116, 101, 114, 115, 32, 105, 110, 32, 71, 101, 114, 109, 97, 110, 121, 32, 102, 117, 110, 100, 101, 100, 32, 117, 110, 100, 101, 114, 32, 116, 104, 101, 32, 102, 101, 100, 101, 114, 97, 108, 32, 103, 111, 118, 101, 114, 110, 109, 637, 116, 226, 128, 153, 115, 32, 65, 73, 32, 115, 116, 114, 97, 116, 101, 103, 121, 32, 98, 121, 32, 116, 104, 101, 32, 70, 101, 100, 101, 114, 97, 108, 32, 77, 105, 110, 105, 115, 116, 114, 121, 32, 111, 102, 32, 69, 100, 117, 99, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 82, 101, 115, 101, 97, 114, 99, 104, 32, 40, 66, 77, 66, 70, 41, 32, 97, 110, 100, 32, 116, 104, 101, 32, 70, 114, 101, 101, 32, 83, 116, 97, 116, 101, 32, 111, 102, 32, 83, 97, 120, 111, 110, 121, 46, 32, 69, 115, 116, 97, 98, 108, 105, 115, 104, 101, 100, 32, 97, 115, 32, 97, 32, 112, 101, 114, 109, 97, 110, 637, 116, 32, 114, 101, 115, 101, 97, 114, 99, 104, 32, 102, 97, 99, 105, 108, 105, 116, 121, 32, 97, 116, 32, 98, 111, 116, 104, 32, 108, 111, 99, 97, 116, 105, 111, 110, 115, 32, 119, 105, 116, 104, 32, 115, 116, 114, 111, 110, 103, 32, 99, 111, 110, 110, 101, 99, 116, 105, 111, 110, 115, 32, 116, 111, 32, 116, 104, 101, 32, 108, 111, 99, 97, 108, 32, 117, 110, 105, 118, 101, 114, 115, 105, 116, 105, 101, 115, 58, 32, 84, 85, 32, 68, 114, 101, 115, 100, 637, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103, 32, 85, 110, 105, 118, 101, 114, 115, 105, 116, 121, 46, 32, 79, 118, 101, 114, 32, 54, 48, 32, 80, 114, 105, 110, 99, 105, 112, 97, 108, 32, 73, 110, 118, 101, 115, 116, 105, 103, 97, 116, 111, 114, 115, 44, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 49, 56, 48, 32, 101, 109, 112, 108, 111, 121, 101, 101, 115, 32, 97, 110, 100, 32, 117, 112, 32, 116, 111, 32, 49, 50, 32, 110, 101, 119, 32, 65, 73, 32, 112, 114, 111, 102, 101, 115, 115, 111, 114, 115, 104, 105, 112, 115, 32, 105, 110, 32, 68, 114, 101, 115, 100, 637, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # TODO: Write a function to merge the given pair of into a new pair with token id 'idx' in 'ids'. \n",
    "  # It should return a list of the new token ids. \n",
    "  # replace all consecutive occurrences of pair with a new idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not a the end and the current id and the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "print(merge([4,3,3,8,1,5], (3,8), 999))\n",
    "print(merge(tokens, top_pair, 637))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 110) to a new token 636\n",
      "merging (101, 114) to a new token 637\n",
      "merging (32, 97) to a new token 638\n",
      "merging (101, 32) to a new token 639\n",
      "merging (110, 100) to a new token 640\n",
      "merging (101, 115) to a new token 641\n",
      "merging (97, 116) to a new token 642\n",
      "merging (97, 108) to a new token 643\n",
      "merging (638, 640) to a new token 644\n",
      "merging (644, 32) to a new token 645\n",
      "merging (105, 103) to a new token 646\n",
      "merging (116, 104) to a new token 647\n",
      "merging (111, 110) to a new token 648\n",
      "merging (643, 32) to a new token 649\n",
      "merging (32, 68) to a new token 650\n",
      "merging (105, 112) to a new token 651\n",
      "merging (115, 32) to a new token 652\n",
      "merging (636, 116) to a new token 653\n",
      "merging (32, 102) to a new token 654\n",
      "merging (111, 114) to a new token 655\n",
      "merging (116, 105) to a new token 656\n",
      "merging (114, 641) to a new token 657\n",
      "merging (105, 110) to a new token 658\n",
      "merging (65, 73) to a new token 659\n",
      "merging (659, 32) to a new token 660\n",
      "merging (99, 105) to a new token 661\n",
      "merging (650, 657) to a new token 662\n",
      "merging (662, 100) to a new token 663\n",
      "merging (663, 636) to a new token 664\n",
      "merging (76, 101) to a new token 665\n",
      "merging (665, 651) to a new token 666\n",
      "merging (666, 122) to a new token 667\n",
      "merging (667, 646) to a new token 668\n",
      "merging (108, 111) to a new token 669\n",
      "merging (111, 102) to a new token 670\n",
      "merging (101, 100) to a new token 671\n",
      "merging (647, 639) to a new token 672\n",
      "merging (118, 637) to a new token 673\n",
      "merging (115, 116) to a new token 674\n",
      "merging (653, 637) to a new token 675\n",
      "merging (32, 83) to a new token 676\n",
      "merging (642, 97) to a new token 677\n",
      "merging (649, 73) to a new token 678\n",
      "merging (678, 110) to a new token 679\n",
      "merging (636, 99) to a new token 680\n",
      "merging (680, 101) to a new token 681\n",
      "merging (647, 32) to a new token 682\n",
      "merging (669, 99) to a new token 683\n",
      "merging (642, 105) to a new token 684\n",
      "merging (684, 648) to a new token 685\n",
      "merging (652, 658) to a new token 686\n",
      "merging (664, 645) to a new token 687\n",
      "merging (687, 668) to a new token 688\n",
      "merging (46, 32) to a new token 689\n",
      "merging (110, 101) to a new token 690\n",
      "merging (97, 110) to a new token 691\n",
      "merging (32, 672) to a new token 692\n",
      "merging (674, 114) to a new token 693\n",
      "merging (675, 654) to a new token 694\n",
      "merging (694, 655) to a new token 695\n",
      "merging (97, 98) to a new token 696\n",
      "merging (696, 108) to a new token 697\n",
      "merging (677, 32) to a new token 698\n",
      "merging (65, 114) to a new token 699\n",
      "merging (699, 656) to a new token 700\n",
      "merging (700, 102) to a new token 701\n",
      "merging (701, 105) to a new token 702\n",
      "merging (702, 661) to a new token 703\n",
      "merging (703, 679) to a new token 704\n",
      "merging (704, 116) to a new token 705\n",
      "merging (705, 101) to a new token 706\n",
      "merging (706, 108) to a new token 707\n",
      "merging (707, 108) to a new token 708\n",
      "merging (708, 646) to a new token 709\n",
      "merging (709, 681) to a new token 710\n",
      "merging (105, 115) to a new token 711\n",
      "merging (638, 32) to a new token 712\n",
      "merging (44, 32) to a new token 713\n",
      "merging (119, 105) to a new token 714\n",
      "merging (714, 682) to a new token 715\n",
      "merging (683, 685) to a new token 716\n",
      "merging (686, 688) to a new token 717\n",
      "merging (689, 79) to a new token 718\n",
      "merging (639, 670) to a new token 719\n",
      "merging (690, 119) to a new token 720\n",
      "merging (720, 32) to a new token 721\n",
      "merging (721, 660) to a new token 722\n",
      "merging (637, 109) to a new token 723\n",
      "merging (723, 691) to a new token 724\n",
      "merging (117, 640) to a new token 725\n",
      "merging (671, 637) to a new token 726\n",
      "merging (726, 649) to a new token 727\n",
      "merging (121, 32) to a new token 728\n",
      "merging (101, 97) to a new token 729\n",
      "merging (729, 114) to a new token 730\n",
      "merging (730, 99) to a new token 731\n",
      "merging (731, 104) to a new token 732\n",
      "merging (105, 116) to a new token 733\n",
      "merging (733, 121) to a new token 734\n",
      "merging (116, 111) to a new token 735\n",
      "merging (110, 105) to a new token 736\n",
      "merging (736, 673) to a new token 737\n",
      "merging (737, 115) to a new token 738\n",
      "merging (48, 32) to a new token 739\n",
      "merging (655, 115) to a new token 740\n",
      "merging (32, 49) to a new token 741\n",
      "merging (83, 99) to a new token 742\n",
      "merging (742, 97) to a new token 743\n",
      "merging (743, 68) to a new token 744\n",
      "merging (744, 83) to a new token 745\n",
      "merging (745, 46) to a new token 746\n",
      "merging (746, 660) to a new token 747\n",
      "merging (747, 40) to a new token 748\n",
      "merging (748, 67) to a new token 749\n",
      "merging (749, 695) to a new token 750\n",
      "merging (750, 676) to a new token 751\n",
      "merging (751, 99) to a new token 752\n",
      "merging (752, 643) to a new token 753\n",
      "merging (753, 697) to a new token 754\n",
      "merging (754, 639) to a new token 755\n",
      "merging (755, 68) to a new token 756\n",
      "merging (756, 698) to a new token 757\n",
      "merging (757, 65) to a new token 758\n",
      "merging (758, 110) to a new token 759\n",
      "merging (759, 643) to a new token 760\n",
      "merging (760, 121) to a new token 761\n",
      "merging (761, 656) to a new token 762\n",
      "merging (762, 99) to a new token 763\n",
      "merging (763, 115) to a new token 764\n",
      "merging (764, 645) to a new token 765\n",
      "merging (765, 710) to a new token 766\n",
      "merging (766, 41) to a new token 767\n",
      "merging (767, 664) to a new token 768\n",
      "merging (768, 47) to a new token 769\n",
      "merging (769, 668) to a new token 770\n",
      "merging (770, 32) to a new token 771\n",
      "merging (771, 711) to a new token 772\n",
      "merging (772, 712) to a new token 773\n",
      "merging (773, 99) to a new token 774\n",
      "merging (774, 695) to a new token 775\n",
      "merging (775, 650) to a new token 776\n",
      "merging (776, 677) to a new token 777\n",
      "merging (777, 676) to a new token 778\n",
      "merging (778, 661) to a new token 779\n",
      "merging (779, 681) to a new token 780\n",
      "merging (780, 713) to a new token 781\n",
      "merging (781, 710) to a new token 782\n",
      "merging (782, 645) to a new token 783\n",
      "merging (783, 66) to a new token 784\n",
      "merging (784, 646) to a new token 785\n",
      "merging (785, 650) to a new token 786\n",
      "merging (786, 698) to a new token 787\n",
      "merging (787, 715) to a new token 788\n",
      "merging (788, 716) to a new token 789\n",
      "merging (789, 717) to a new token 790\n",
      "merging (790, 718) to a new token 791\n",
      "merging (791, 110) to a new token 792\n",
      "merging (792, 719) to a new token 793\n",
      "merging (793, 654) to a new token 794\n",
      "merging (794, 105) to a new token 795\n",
      "merging (795, 118) to a new token 796\n",
      "merging (796, 639) to a new token 797\n",
      "merging (797, 722) to a new token 798\n",
      "merging (798, 99) to a new token 799\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 800 # the desired size of the vocabulary\n",
    "token_size = len(tokens) # the current size of the vocabulary\n",
    "num_merges = vocab_size - token_size # the number of merges we need to make\n",
    "ids = list(tokens) # the current vocabulary, so we don't destroy the original list\n",
    "\n",
    "for i in range(num_merges):\n",
    "  # TODO: Code the for loop through the number of merges.\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get) # get the most common pair by value\n",
    "  idx = token_size + i # the new id\n",
    "  print(f\"merging {pair} to a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length:  636\n",
      "ids length:  185\n",
      "[83, 99, 97, 68, 83, 46, 65, 73, 32, 40, 67, 101, 110, 116, 101, 114, 32, 102, 111, 114, 32, 83, 99, 97, 108, 97, 98, 108, 101, 32, 68, 97, 116, 97, 32, 65, 110, 97, 108, 121, 116, 105, 99, 115, 32, 97, 110, 100, 32, 65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 73, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101, 41, 32, 68, 114, 101, 115, 100, 101, 110, 47, 76, 101, 105, 112, 122, 105, 103, 32, 105, 115, 32, 97, 32, 99, 101, 110, 116, 101, 114, 32, 102, 111, 114, 32, 68, 97, 116, 97, 32, 83, 99, 105, 101, 110, 99, 101, 44, 32, 65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 73, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101, 32, 97, 110, 100, 32, 66, 105, 103, 32, 68, 97, 116, 97, 32, 119, 105, 116, 104, 32, 108, 111, 99, 97, 116, 105, 111, 110, 115, 32, 105, 110, 32, 68, 114, 101, 115, 100, 101, 110, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103, 46, 32, 79, 110, 101, 32, 111, 102, 32, 102, 105, 118, 101, 32, 110, 101, 119, 32, 65, 73, 32, 99, 101, 110, 116, 101, 114, 115, 32, 105, 110, 32, 71, 101, 114, 109, 97, 110, 121, 32, 102, 117, 110, 100, 101, 100, 32, 117, 110, 100, 101, 114, 32, 116, 104, 101, 32, 102, 101, 100, 101, 114, 97, 108, 32, 103, 111, 118, 101, 114, 110, 109, 101, 110, 116, 226, 128, 153, 115, 32, 65, 73, 32, 115, 116, 114, 97, 116, 101, 103, 121, 32, 98, 121, 32, 116, 104, 101, 32, 70, 101, 100, 101, 114, 97, 108, 32, 77, 105, 110, 105, 115, 116, 114, 121, 32, 111, 102, 32, 69, 100, 117, 99, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 82, 101, 115, 101, 97, 114, 99, 104, 32, 40, 66, 77, 66, 70, 41, 32, 97, 110, 100, 32, 116, 104, 101, 32, 70, 114, 101, 101, 32, 83, 116, 97, 116, 101, 32, 111, 102, 32, 83, 97, 120, 111, 110, 121, 46, 32, 69, 115, 116, 97, 98, 108, 105, 115, 104, 101, 100, 32, 97, 115, 32, 97, 32, 112, 101, 114, 109, 97, 110, 101, 110, 116, 32, 114, 101, 115, 101, 97, 114, 99, 104, 32, 102, 97, 99, 105, 108, 105, 116, 121, 32, 97, 116, 32, 98, 111, 116, 104, 32, 108, 111, 99, 97, 116, 105, 111, 110, 115, 32, 119, 105, 116, 104, 32, 115, 116, 114, 111, 110, 103, 32, 99, 111, 110, 110, 101, 99, 116, 105, 111, 110, 115, 32, 116, 111, 32, 116, 104, 101, 32, 108, 111, 99, 97, 108, 32, 117, 110, 105, 118, 101, 114, 115, 105, 116, 105, 101, 115, 58, 32, 84, 85, 32, 68, 114, 101, 115, 100, 101, 110, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103, 32, 85, 110, 105, 118, 101, 114, 115, 105, 116, 121, 46, 32, 79, 118, 101, 114, 32, 54, 48, 32, 80, 114, 105, 110, 99, 105, 112, 97, 108, 32, 73, 110, 118, 101, 115, 116, 105, 103, 97, 116, 111, 114, 115, 44, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 49, 56, 48, 32, 101, 109, 112, 108, 111, 121, 101, 101, 115, 32, 97, 110, 100, 32, 117, 112, 32, 116, 111, 32, 49, 50, 32, 110, 101, 119, 32, 65, 73, 32, 112, 114, 111, 102, 101, 115, 115, 111, 114, 115, 104, 105, 112, 115, 32, 105, 110, 32, 68, 114, 101, 115, 100, 101, 110, 32, 97, 110, 100, 32, 76, 101, 105, 112, 122, 105, 103]\n",
      "[799, 675, 686, 32, 71, 724, 121, 654, 725, 671, 32, 725, 637, 692, 102, 727, 103, 111, 673, 110, 109, 653, 226, 128, 153, 652, 660, 693, 642, 101, 103, 728, 98, 121, 692, 70, 727, 77, 658, 105, 693, 728, 670, 32, 69, 100, 117, 99, 685, 645, 82, 641, 732, 32, 40, 66, 77, 66, 70, 41, 645, 672, 70, 114, 101, 639, 83, 116, 642, 719, 676, 97, 120, 648, 121, 689, 69, 674, 697, 711, 104, 671, 638, 115, 712, 112, 724, 653, 32, 657, 732, 654, 97, 661, 108, 734, 638, 116, 32, 98, 111, 682, 716, 652, 715, 693, 648, 103, 32, 99, 648, 690, 99, 656, 648, 652, 735, 692, 683, 649, 117, 738, 105, 656, 641, 58, 32, 84, 85, 688, 32, 85, 738, 734, 718, 673, 32, 54, 739, 80, 114, 658, 99, 651, 679, 118, 641, 116, 646, 642, 740, 713, 109, 655, 639, 647, 691, 741, 56, 739, 101, 109, 112, 669, 121, 101, 641, 645, 117, 112, 32, 735, 741, 50, 32, 722, 112, 114, 670, 641, 115, 740, 104, 651, 717]\n",
      "Compression ratio: 3.44X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length: \", len(tokens))\n",
    "print(\"ids length: \", len(ids))\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(f\"Compression ratio: {len(tokens) / len(ids):.2f}X\") # Compromise between token length and vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Vocabulary size: 800\n",
      "New Vocabulary size: 102\n"
     ]
    }
   ],
   "source": [
    "new_vocab_size = len(set(ids))\n",
    "print(f\"Old Vocabulary size: {vocab_size}\")\n",
    "print(f\"New Vocabulary size: {new_vocab_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
