{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization is at the heart of much weirdness of LLMs. \n",
    "\n",
    "You can play around with different kind of tokenizers here in [this website](https://tiktokenizer.vercel.app/).\n",
    "Check out how code an foreign languages are represented in different tokenizers. Check how spaces in python are handled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strings** are just numbers encoded by a standard text encoding scheme like unicode. To get the unicode number representation for a number we can use the `ord` function in python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l: 108\n",
      "ü´®: 129768\n",
      "[104, 105, 32, 116, 104, 101, 114, 101, 32, 128075, 127996]\n"
     ]
    }
   ],
   "source": [
    "print(\"l: \" + str(ord(\"l\")))\n",
    "print(\"ü´®: \" + str(ord(\"ü´®\")))\n",
    "print([ord(c) for c in \"hi there üëãüèº\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode supports about 150k such numbers. Adding support to all the different characters supported by Unicode will massively increase the input to our language model. Standards can also change. We primarily use unicode (utf-8) encoding because it's fairly concise in terms of the size of the representation for english and it's backwards compatible with ASCII. This would also count as a character tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before implementing BPE tokenization, we want to start with simple whitespace tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "text = \"\"\"ScaDS.AI (Center for Scalable Data Analytics and Artificial Intelligence) Dresden/Leipzig is a center for Data Science, Artificial Intelligence and Big Data with locations in Dresden and Leipzig. One of five new AI centers in Germany funded under the federal government‚Äôs AI strategy by the Federal Ministry of Education and Research (BMBF) and the Free State of Saxony. Established as a permanent research facility at both locations with strong connections to the local universities: TU Dresden and Leipzig University. Over 60 Principal Investigators, more than 180 employees and up to 12 new AI professorships in Dresden and Leipzig\"\"\"\n",
    "\n",
    "# TODO: Split the text by whitespace\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    # TODO: Convert into lowercase and remove punctuation\n",
    "    return tokens\n",
    "\n",
    "word_tokens = simple_tokenizer(text)\n",
    "print(\"Tokens after preprocessing:\", word_tokens)\n",
    "\n",
    "# TODO: Calculate unigram frequencies - hint: you can use the Counter Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword-Level-Tokenization - Byte Pair Encoding (BPE) algorithm\n",
    "\n",
    "Feeding raw UTF-8 would be really nice, but the downside to that is the long context length. Remember our model has a limited amount of memory, thus we need to compress the raw text input into something smaller (ideally variable length) - which still retains the same information as our original text. Also, splitting text by whitespaces is very easy, but it does not work well with rare, Out-Of-Bag (OOB) words. \n",
    "\n",
    "The classic idea of text compression (Huffman Coding) says we put more effort (more characters, more memory) to represent characters which are rare in our sequence and characters or rather sequence of characters which repeat a lot can be represented with shorter symbols or less memory\n",
    "\n",
    "The BPE algorithm follows from the same idea. For example\n",
    "Suppose we have the following string:\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "The byte pair \"aa\" occurs most often, so it will be replaced by a bute that is not used in the data, such as \"Z\". Now there is the following data and replacement table\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "Then this process is repeated. We keep minting new tokens (symbols) to replace old symbols which repeat frequently\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "(Much like how we expand grammar in formal languages)\n",
    "\n",
    "And finally\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "**Vocabulary** refers to the number of unique symbols we use to represent our text. For decimal number system with base 10 the vocabulary is 10 (0-9) and for english the vocabulary of characters is 26 (a-z). Note that by replacing `Z=aa` we effectively reduced the length of our string but the vocabulary size has increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"ScaDS.AI (Center for Scalable Data Analytics and Artificial Intelligence) Dresden/Leipzig is a center for Data Science, Artificial Intelligence and Big Data with locations in Dresden and Leipzig. One of five new AI centers in Germany funded under the federal government‚Äôs AI strategy by the Federal Ministry of Education and Research (BMBF) and the Free State of Saxony. Established as a permanent research facility at both locations with strong connections to the local universities: TU Dresden and Leipzig University. Over 60 Principal Investigators, more than 180 employees and up to 12 new AI professorships in Dresden and Leipzig\"\"\"\n",
    "\n",
    "bytes = text.encode('utf-8')\n",
    "tokens = [int(token) for token in bytes]\n",
    "vocab_size = len(set(tokens))\n",
    "print(\"Vocabulary Size: \" + str(vocab_size))\n",
    "mapping = {token: chr(token) for token in tokens}\n",
    "print(\"length of text:\", len(text))\n",
    "print(\"Tokens from UTF-8 Values:\")\n",
    "print(tokens)\n",
    "print(\"length of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to count which pairs of characters are occuring the most amount of time. So we write a function for the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    # TODO: Implement the function body. It should return a dictionary with each pair as key and the frequency as its value. \n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "print(sorted(((v, k) for k,v in stats.items()), reverse=True)) # sort by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to merge this pair so we write a merge function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # TODO: Write a function to merge the given pair of into a new pair with token id 'idx' in 'ids'. \n",
    "  # It should return a list of the new token ids. \n",
    "  # replace all consecutive occurrences of pair with a new idx\n",
    "  return newids\n",
    "\n",
    "print(merge([4,3,3,8,1,5], (3,8), 999))\n",
    "print(merge(tokens, top_pair, 637))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 800 # the desired size of the vocabulary\n",
    "token_size = len(tokens) # the current size of the vocabulary\n",
    "num_merges = vocab_size - token_size # the number of merges we need to make\n",
    "ids = list(tokens) # the current vocabulary, so we don't destroy the original list\n",
    "\n",
    "for i in range(num_merges):\n",
    "  # TODO: Code the for loop through the number of merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokens length: \", len(tokens))\n",
    "print(\"ids length: \", len(ids))\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(f\"Compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_size = len(set(ids))\n",
    "print(f\"Old Vocabulary size: {vocab_size}\")\n",
    "print(f\"New Vocabulary size: {new_vocab_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
